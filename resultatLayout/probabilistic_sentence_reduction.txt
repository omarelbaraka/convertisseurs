Probabilistic Sentence Reduction Using Support Vector Machines
                 Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi
                          Bao Tu Ho and Masaru Fukushi

                     Japan Advanced Institute of Science and Technology
                        1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN
                     {nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp


                   Abstract                           As pointed out by Lin (Lin 03), the best sen-
                                                      tence reduction output for a single sentence is
This paper investigates a novel application of sup-
                                                      not approximately best for text summarization.
port vector machines (SVMs) for sentence reduction.
                                                      This means that “local optimal” refers to the
We also propose a new probabilistic sentence reduc-
                                                      best reduced output for a single sentence, while
tion method based on support vector machine learn-
                                                      the best reduced output for the whole text is
ing. Experimental results show that the proposed
                                                      “global optimal”. Thus, it would be very valu-
methods outperform earlier methods in term of sen-
                                                      able if the sentence reduction task could gener-
tence reduction performance.
                                                      ate multiple reduced outputs and select the best
                                                      one using the whole text document. However,
1   Introduction
                                                      such a sentence reduction method has not yet
The most popular methods of sentence reduc-           been proposed.
tion for text summarization are corpus based             Support Vector Machines (Vapnik 95), on the
methods. Jing (Jing 00) developed a method            other hand, are strong learning methods in com-
to remove extraneous phrases from sentences           parison with decision tree learning and other
by using multiple sources of knowledge to de-         learning methods (Sholkopf 97). The goal of
cide which phrases could be removed. However,         this paper is to illustrate the potential of SVMs
while this method exploits a simple model for         for enhancing the accuracy of sentence reduc-
sentence reduction by using statistics computed       tion in comparison with previous work. Accord-
from a corpus, a better model can be obtained         ingly, we describe a novel deterministic method
by using a learning approach.                         for sentence reduction using SVMs and a two-
   Knight and Marcu (Knight and Marcu 02)             stage method using pairwise coupling (Hastie
proposed a corpus based sentence reduction            98). To solve the problem of generating mul-
method using machine learning techniques.             tiple best outputs, we propose a probabilistic
They discussed a noisy-channel based approach         sentence reduction model, in which a variant of
and a decision tree based approach to sentence        probabilistic SVMs using a two-stage method
reduction. Their algorithms provide the best          with pairwise coupling is discussed.
way to scale up the full problem of sentence re-         The rest of this paper will be organized as
duction using available data. However, these al-      follows: Section 2 introduces the Support Vec-
gorithms require that the word order of a given       tor Machines learning. Section 3 describes the
sentence and its reduced sentence are the same.       previous work on sentence reduction and our
Nguyen and Horiguchi (Nguyen and Horiguchi            deterministic sentence reduction using SVMs.
03) presented a new sentence reduction tech-          We also discuss remaining problems of deter-
nique based on a decision tree model without          ministic sentence reduction. Section 4 presents
that constraint. They also indicated that se-         a probabilistic sentence reduction method using
mantic information is useful for sentence reduc-      support vector machines to solve this problem.
tion tasks.                                           Section 5 discusses implementation and our ex-
   The major drawback of previous works on            perimental results; Section 6 gives our conclu-
sentence reduction is that those methods are          sions and describes some problems that remain
likely to output local optimal results, which may     to be solved in the future.
have lower accuracy. This problem is caused by
the inherent sentence reduction model; that is,
only a single reduced sentence can be obtained.
2   Support Vector Machine                                    that consists of sub trees in order to rewrite a
Support vector machine (SVM)(Vapnik 95) is a                  small tree. Let RSTACK be a stack that con-
technique of machine learning based on statisti-              sists of sub trees which are removed from the
cal learning theory. Suppose that we are given                Input list in the rewriting process.
l training examples (xi , yi ), (1 ≤ i ≤ l), where
xi is a feature vector in n dimensional feature                 • SHIFT action transfers the first word from the
space, yi is the class label {-1, +1 } of xi . SVM                Input list into CSTACK. It is written mathe-
finds a hyperplane w.x + b = 0 which correctly                    matically and given the label SHIFT.
separates the training examples and has a max-                  • REDUCE(lk, X) action pops the lk syntactic
imum margin which is the distance between two                     trees located at the top of CSTACK and com-
hyperplanes w.x + b ≥ 1 and w.x + b ≤ −1. The                     bines them in a new tree, where lk is an integer
optimal hyperplane with maximum margin can                        and X is a grammar symbol.
be obtained by solving the following quadratic
programming.                                                    • DROP X action moves subsequences of words
                                                                  that correspond to syntactic constituents from
                       1
                                     P
                                     l
                                                                  the Input list to RSTACK.
           min         2 kwk + C0         ξi
                                      i                 (1)     • ASSIGN TYPE X action changes the label of
           s.t.   yi (w.xi + b) ≥ 1 − ξi
                                                                  trees at the top of the CSTACK. These POS
           ξi ≥ 0
                                                                  tags might be different from the POS tags in
 where C0 is the constant and ξi is a slack vari-                 the original sentence.
able for the non-separable case. In SVM, the                    • RESTORE X action takes the X element in
optimal hyperplane is formulated as follows:                      RSTACK and moves it into the Input list,
                       Ã l                          !             where X is a subtree.
                        X
        f (x) = sign          αi yi K(xi , x) + b       (2)
                          1                                   For convenience, let configuration be a status
                                                              of Input list, CSTACK and RSTACK. Let cur-
  where αi is the Lagrange multiple, and                      rent context be the important information in a
K(x0 , x00 ) is a kernel function, the SVM calcu-             configuration. The important information are
lates similarity between two arguments x0 and
x00 . For instance, the Polynomial kernel func-               defined as a vector of features using heuristic
tion is formulated as follow:                                 methods as in (Knight and Marcu 02), (Nguyen
                                                              and Horiguchi 03).
                 K(x0 , x00 ) = (x0 .x00 )p             (3)      The main idea behind deterministic sentence
                                                              reduction is that it uses a rule in the current
 SVMs estimate the label of an unknown ex-
                                                              context of the initial configuration to select a
ample x whether the sign of f (x) is positive or
                                                              distinct action in order to rewrite an input sen-
not.
                                                              tence into a reduced sentence. After that, the
3   Deterministic Sentence Reduction                          current context is changed to a new context and
    Using SVMs                                                the rewriting process is repeated for selecting
                                                              an action that corresponds to the new context.
3.1 Problem Description
                                                              The rewriting process is finished when it meets
In the corpus-based decision tree approach, a                 a termination condition. Here, one rule corre-
given input sentence is parsed into a syntax tree             sponds to the function that maps the current
and the syntax tree is then transformed into a                context to a rewriting action. These rules are
small tree to obtain a reduced sentence.                      learned automatically from the corpus of long
   Let t and s be syntax trees of the original sen-           sentences and their reduced sentences (Knight
tence and a reduced sentence, respectively. The               and Marcu 02), (Nguyen and Horiguchi 03).
process of transforming syntax tree t to small
tree s is called “rewriting process” (Knight and              3.2 Example
Marcu 02), (Nguyen and Horiguchi 03). To                      Figure 1 shows an example of applying a se-
transform the syntax tree t to the syntax tree                quence of actions to rewrite the input sentence
s, some terms and five rewriting actions are de-              (a, b, c, d, e), when each character is a word. It
fined.                                                        illustrates the structure of the Input list, two
   An Input list consists of a sequence of words              stacks, and the term of a rewriting process based
subsumed by the tree t where each word in the                 on the actions mentioned above. For example,
Input list is labelled with the name of all syntac-           in the first row, DROP H deletes the sub-tree
tic constituents in t. Let CSTACK be a stack                  with its root node H in the Input list and stores
it in the RSTACK. The reduced tree s can be
obtained after applying a sequence of actions
as follows: DROP H; SHIFT; ASSIGN TYPE K;
DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-
DUCE 2G. In this example, the reduced sentence
is (b, e, a).




                                                           Figure 2: Example of Configuration

                                                    that start with the first unit in the Input list.
                                                    For example, in Figure 2 the syntactic con-
                                                    stituents are labels of the current element in the
                                                    Input list from “VP” to the verb “convince”.
                                                    Semantic features
                                                    The following features are used in our model as
 Figure 1: An Example of the Rewriting Process      semantic information.

3.3   Learning Reduction Rules Using                  • Semantic information about current words
      SVMs                                              within the Input list; these semantic types
                                                        are obtained by using the named entities such
As mentioned above, the action for each config-         as Location, Person, Organization and Time
uration can be decided by using a learning rule,        within the input sentence. To define these
which maps a context to an action. To obtain            name entities, we use the method described in
such rules, the configuration is represented by         (Borthwick 99).
a vector of features with a high dimension. Af-
ter that, we estimate the training examples by        • Semantic information about whether or not the
using several support vector machines to deal           word in the Input list is a head word.
with the multiple classification problem in sen-
                                                      • Word relations, such as whether or not a word
tence reduction.
                                                        has a relationship with other words in the sub-
3.3.1 Features                                          categorization table. These relations and the
One important task in applying SVMs to text             sub-categorization table are obtained using the
summarization is to define features. Here, we           Commlex database (Macleod 95).
describe features used in our sentence reduction
models.                                             Using the semantic information, we are able to
   The features are extracted based on the cur-     avoid deleting important segments within the
rent context. As it can be seen in Figure 2, a      given input sentence. For instance, the main
context includes the status of the Input list and   verb, the subject and the object are essential
the status of CSTACK and RSTACK. We de-             and for the noun phrase, the head noun is essen-
fine a set of features for a current context as     tial, but an adjective modifier of the head noun
described bellow.                                   is not. For example, let us consider that the
Operation feature                                   verb “convince” was extracted from the Com-
The set of features as described in (Nguyen and     lex database as follows.
Horiguchi 03) are used in our sentence reduction      convince
models.                                               NP-PP: PVAL (“of”)
Original tree features                                NP-TO-INF-OC
These features denote the syntactic constituents    This entry indicates that the verb “convince”
can be followed by a noun phrase and a preposi-
                                                      Table 1: Distribution of example data on five
tional phrase starting with the preposition “of”.
                                                      data groups
It can be also followed by a noun phrase and a            Name                    Number of examples
to-infinite phrase. This information shows that           SHIFT-GROUP                  13,363
we cannot delete an “of” prepositional phrase             REDUCE-GROUP                 11,406
or a to-infinitive that is the part of the verb           DROP-GROUP                    4,216
phrase.                                                   ASSIGN-GROUP                 13,363
3.3.2    Two-stage SVM Learning using                     RESTORE-GROUP                 2,004
         Pairwise Coupling                                TOTAL                        44,352
Using these features we can extract training
data for SVMs. Here, a sample in our training
data consists of pairs of a feature vector and        tions, then the possibility of obtaining a wrong
an action. The algorithm to extract training          reduced output becomes high.
data from the training corpus is modified using          One way to solve this problem is to select mul-
the algorithm described in our pervious work          tiple actions that correspond to the context at
(Nguyen and Horiguchi 03).                            each step in the rewriting process. However,
   Since the original support vector machine          the question that emerges here is how to deter-
(SVM) is a binary classification method, while        mine which criteria to use in selecting multiple
the sentence reduction problem is formulated as       actions for a context. If this problem can be
multiple classification, we have to find a method     solved, then multiple best reduced outputs can
to adapt support vector machines to this prob-        be obtained for each input sentence and the best
lem. For multi-class SVMs, one can use strate-        one will be selected by using the whole text doc-
gies such as one-vs all, pairwise comparison or       ument.
DAG graph (Hsu 02). In this paper, we use the            In the next section propose a model for se-
pairwise strategy, which constructs a rule for        lecting multiple actions for a context in sentence
discriminating pairs of classes and then selects      reduction as a probabilistic sentence reduction
the class with the most winning among two class       and present a variant of probabilistic sentence
decisions.                                            reduction.
   To boost the training time and the sentence
                                                      4   Probabilistic Sentence Reduction
reduction performance, we propose a two-stage
SVM described below.
                                                          Using SVM
   Suppose that the examples in training data         4.1      The Probabilistic SVM Models
are divided into five groups m1 , m2 , ..., m5 ac-    Let A be a set of k actions A =
cording to their actions. Let Svmc be multi-          {a1 , a2 ...ai , ..., ak } and C be a set of n con-
class SVMs and let Svmc-i be multi-class SVMs         texts C = {c1 , c2 ...ci , ..., cn } . A probabilistic
for a group mi . We use one Svmc classifier to        model α for sentence reduction will select an
identify the group to which a given context e         action a ∈ A for the context c with probability
should be belong. Assume that e belongs to            pα (a|c). The pα (a|c) can be used to score ac-
the group mi . The classifier Svmc-i is then used     tion a among possible actions A depending the
to recognize a specific action for the context e.     context c that is available at the time of deci-
The five classifiers Svmc-1, Svmc-2,..., Svmc-5       sion. There are several methods for estimating
are trained by using those examples which have        such scores; we have called these “probabilistic
actions belonging to SHIFT, REDUCE, DROP,             sentence reduction methods”. The conditional
ASSIGN TYPE and RESTORE.                              probability pα (a|c) is estimated using a variant
   Table 1 shows the distribution of examples in      of probabilistic support vector machine, which
five data groups.                                     is described in the following sections.
3.4   Disadvantage of Deterministic                   4.1.1 Probabilistic SVMs using
      Sentence Reductions                                        Pairwise Coupling
                                                      For convenience, we denote uij = p(a = ai |a =
The idea of the deterministic algorithm is to         ai ∨aj , c). Given a context c and an action a, we
use the rule for each current context to select       assume that the estimated pairwise class prob-
the next action, and so on. The process termi-        abilities rij of uij are available. Here rij can
nates when a stop condition is met. If the early      be estimated by some binary classifiers. For
steps of this algorithm fail to select the best ac-   instance, we could estimate rij by using the
SVM binary posterior probabilities as described                               the search procedure ranks the derivation of in-
in (Plat 2000). Then, the goal is to estimate                                 complete and complete reduced sentences. Let
{pi }ki=1 , where pi = p(a = ai |c), i = 1, 2, ..., k.                        d(s) = {a1 , a2 , ...ad } be the derivation of a small
For this propose, a simple estimate of these                                  tree s, where each action ai belongs to a set of
probabilities can be derived using the following                              possible actions. The score of s is the product
voting method:                                                                of the conditional probabilities of the individual
                           X                                                  actions in its derivation.
                pi = 2            I{rij >rji } /k(k − 1)                                                    Y
                                                                                           Score(s) =              p(ai |ci )   (6)
                         j:j6=i
                                                                                                        ai ∈d(s)
where I is an indicator function and k(k − 1) is
                                                                              where ci is the context in which ai was decided.
the number of pairwise classes. However, this                                 The search heuristic tries to find the best re-
model is too simple; we can obtain a better one                               duced tree s∗ as follows:
with the following method.
   Assume that uij are pairwise probabilities of                                           s∗ = argmax       Score(s)           (7)
                                                                                                | {z }
the model subject to the condition that uij =                                                   s∈tree(t)
pi /(pi +pj ). In (Hastie 98), the authors proposed
to minimize the Kullback-Leibler (KL) distance                                where tree(t) are all the complete reduced trees
between the rij and uij                                                       from the tree t of the given long sentence. As-
                                  X                    rij                    sume that for each configuration the actions
                    l(p) =               nij rij log                    (4)   {a1 , a2 , ...an } are sorted in decreasing order ac-
                                                       uij
                                  i6=j                                        cording to p(ai |ci ), in which ci is the context
where rij and uij are the probabilities of a pair-                            of that configuration. Algorithm 1 shows a
wise ai and aj in the estimated model and in                                  probabilistic sentence reduction using the top
our model, respectively, and nij is the number                                K-BFS search algorithm. This algorithm uses
of training data in which their classes are ai or                             a breadth-first search which does not expand
aj . To find the minimizer of equation (6), they                              the entire frontier, but instead expands at most
first calculate                                                               the top K scoring incomplete configurations in
               ∂l(p) X        rij      1                                      the frontier; it is terminated when it finds M
                    =  nij (−     +         ).
                ∂pi           pi    pi + pj                                   completed reduced sentences (CL is a list of re-
                           i6=j
                                                                              duced trees), or when all hypotheses have been
 Thus, letting ∆l(p) = 0, they proposed to find                               exhausted. A configuration is completed if and
a point satisfying                                                            only if the Input list is empty and there is one
      X                    X                            P
                                                        k                     tree in the CSTACK. Note that the function
               nij uij =            nij rij ,                 pi = 1,         get-context(hi , j) obtains the current context of
      j:j6=i               j:j6=i                       i=1
                                                                              the j th configuration in hi , where hi is a heap at
where i = 1, 2, ...k and pi > 0.                                              step i. The function Insert(s,h) ensures that the
Such a point can be obtained by using an algo-                                heap h is sorted according to the score of each
rithm described elsewhere in (Hastie 98). We                                  element in h. Essentially, in implementation we
applied it to obtain a probabilistic SVM model                                can use a dictionary of contexts and actions ob-
for sentence reduction using a simple method as                               served from the training data in order to reduce
follows. Assume that our class labels belong to                               the number of actions to explore for a current
l groups: M = {m1 , m2 ...mi , ..., ml } , where l                            context.
is a number of groups and mi is a group e.g.,
SHIFT, REDUCE ,..., ASSIGN TYPE. Then                                         5   Experiments and Discussion
the probability p(a|c) of an action a for a given
context c can be estimated as follows.                                        We used the same corpus as described in
                                                                              (Knight and Marcu 02), which includes 1,067
                 p(a|c) = p(mi |c) × p(a|c, mi )                        (5)   pairs of sentences and their reductions. To
                                                                              evaluate sentence reduction algorithms, we ran-
where mi is a group and a ∈ mi . Here, p(mi |c)
                                                                              domly selected 32 pairs of sentences from our
and p(a|c, mi ) are estimated by the method in
                                                                              parallel corpus, which is refered to as the test
(Hastie 98).
                                                                              corpus. The training corpus of 1,035 sentences
4.2   Probabilistic sentence reduction                                        extracted 44,352 examples, in which each train-
      algorithm                                                               ing example corresponds to an action. The
After obtaining a probabilistic model p, we then                              SVM tool, LibSVM (Chang 01) is applied to
use this model to define function score, by which                             train our model. The training examples were
Algorithm 1 A probabilistic sentence reduction                      reduced sentences.
algorithm                                                              Table 2 shows the results of English language
 1: CL={Empty};                                                     sentence reduction using a support vector ma-
      i = 0; h0 ={ Initial configuration}                           chine compared with the baseline methods and
 2: while |CL| < M do                                               with human reduction. Table 2 shows compres-
 3:  if hi is empty then                                            sion rates, and mean and standard deviation re-
 4:    break;
 5:  end if                                                         sults across all judges, for each algorithm. The
 6:  u =min(|hi |, K)
 7:  for j = 1 to u do                                              results show that the length of the reduced sen-
 8:    c=get-context(hi , j)                                        tences using decision trees is shorter than using
                              P
                              m
 9:        Select m so that         p(ai |c) < Q is maximal         SVMs, and indicate that our new methods out-
10:        for l=1 to m do
                              i=1
                                                                    perform the baseline algorithms in grammatical
11:          parameter=get-parameter(al );                          and importance criteria. Table 2 shows that the
12:          Obtain a new configuration s by performing action al
             with parameter
13:       if Complete(s) then
14:          Insert(s, CL)                                          Table 2: Experiment results with Test Corpus
15:       else                                                        Method       Comp    Gramma          Impo
16:          Insert(s, hi+1 )
17:       end if
                                                                      Baseline1   57.19%   8.60 ± 2.8   7.18 ± 1.92
18:     end for                                                       Baseline2   57.15%   8.60 ± 2.1   7.42 ± 1.90
19:   end for
20:   i=i+1                                                           SVM-D       57.65%   8.76 ± 1.2   7.53 ± 1.53
21: end while                                                         SVMP-10     57.51%   8.80 ± 1.3   7.74 ± 1.39
                                                                      Human       64.00%   9.05 ± 0.3   8.50 ± 0.80

divided into SHIFT, REDUCE, DROP, RE-
STORE, and ASSIGN groups. To train our                              first 10 reduced sentences produced by SVMP-
support vector model in each group, we used                         10 (the SVM probabilistic model) obtained the
the pairwise method with the polynomial ker-                        highest performances. We also compared the
nel function, in which the parameter p in (3)                       computation time of sentence reduction using
and the constant C0 in equation (1) are 2 and                       support vector machine with that in previous
0.0001, respectively.                                               works. Table 3 shows that the computational
                                                                    times for SVM-D and SVMP-10 are slower than
   The algorithms (Knight and Marcu 02) and
                                                                    baseline, but it is acceptable for SVM-D.
(Nguyen and Horiguchi 03) served as the base-
line1 and the baseline2 for comparison with the
proposed algorithms. Deterministic sentence re-                     Table 3: Computational times of performing re-
duction using SVM and probabilistic sentence                        ductions on test-set. Average sentence length
reduction were named as SVM-D and SVMP, re-                         was 21 words.
                                                                         Method      Computational times (sec)
spectively. For convenience, the ten top reduced
                                                                         Baseline1           138.25
outputs using SVMP were called SVMP-10. We                               SVM-D               212.46
used the same evaluation method as described                             SVMP-10            1030.25
in (Knight and Marcu 02) to compare the pro-
posed methods with previous methods. For this
experiment, we presented each original sentence                        We also investigated how sensitive the pro-
in the test corpus to three judges who are spe-                     posed algorithms are with respect to the train-
cialists in English, together with three sentence                   ing data by carrying out the same experi-
reductions: the human generated reduction sen-                      ment on sentences of different genres. We
tence, the outputs of the proposed algorithms,                      created the test corpus by selecting sentences
and the output of the baseline algorithms.                          from the web-site of the Benton Foundation
   The judges were told that all outputs were                       (http://www.benton.org). The leading sen-
generated automatically. The order of the out-                      tences in each news article were selected as the
puts was scrambled randomly across test cases.                      most relevant sentences to the summary of the
The judges participated in two experiments. In                      news. We obtained 32 leading long sentences
the first, they were asked to determine on a scale                  and 32 headlines for each item. The 32 sen-
from 1 to 10 how well the systems did with re-                      tences are used as a second test for our methods.
spect to selecting the most important words in                      We use a simple ranking criterion: the more the
the original sentence. In the second, they were                     words in the reduced sentence overlap with the
asked to determine the grammatical criteria of                      words in the headline, the more important the
sentence is. A sentence satisfying this criterion    References
is called a relevant candidate.                      A. Borthwick, “A Maximum Entropy Approach
   For a given sentence, we used a simple               to Named Entity Recognition”, Ph.D the-
method, namely SVMP-R to obtain a re-                   sis, Computer Science Department, New York
duced sentence by selecting a relevant candi-           University (1999).
date among the ten top reduced outputs using         C.-C. Chang and C.-J. Lin,                 “LIB-
SVMP-10.                                                SVM: a library for support vec-
   Table 4 depicts the experiment results for           tor machines”, Software available at
the baseline methods, SVM-D, SVMP-R, and                http://www.csie.ntu.edu.tw/ cjlin/libsvm.
SVMP-10. The results shows that, when ap-            H. Jing, “Sentence reduction for automatic
plied to sentence of a different genre, the per-        text summarization”, In Proceedings of the
formance of SVMP-10 degrades smoothly, while            First Annual Meeting of the North Ameri-
the performance of the deterministic sentence           can Chapter of the Association for Compu-
reductions (the baselines and SVM determinis-           tational Linguistics NAACL-2000.
tic) drops sharply. This indicates that the prob-    T.T. Hastie and R. Tibshirani, “Classification
abilistic sentence reduction using support vector       by pairwise coupling”, The Annals of Statis-
machine is more stable.                                 tics, 26(1): pp. 451-471, 1998.
   Table 4 shows that the performance of
                                                     C.-W. Hsu and C.-J. Lin, “A comparison of
SVMP-10 is also close to the human reduction
                                                        methods for multi-class support vector ma-
outputs and is better than previous works. In
                                                        chines”, IEEE Transactions on Neural Net-
addition, SVMP-R outperforms the determin-
                                                        works, 13, pp. 415-425, 2002.
istic sentence reduction algorithms and the dif-
                                                     K. Knight and D. Marcu, “Summarization be-
ferences between SVMP-R’s results and SVMP-
                                                        yond sentence extraction: A Probabilistic ap-
10 are small. This indicates that we can ob-
                                                        proach to sentence compression”, Artificial
tain reduced sentences which are relevant to the
                                                        Intelligence 139: pp. 91-107, 2002.
headline, while ensuring the grammatical and
                                                     C.Y. Lin, “Improving Summarization Perfor-
the importance criteria compared to the origi-
                                                        mance by Sentence Compression — A Pi-
nal sentences.
                                                        lot Study”, Proceedings of the Sixth Inter-
                                                        national Workshop on Information Retrieval
Table 4: Experiment results with Benton Cor-            with Asian Languages, pp.1-8, 2003.
pus
                                                     C. Macleod and R. Grishman, “COMMLEX
    Method      Comp      Gramma          Impo
    Baseline1   54.14%   7.61 ± 2.10   6.74 ± 1.92      syntax Reference Manual”; Proteus Project,
    Baseline2   53.13%   7.72 ± 1.60   7.02 ± 1.90      New York University (1995).
    SVM-D       56.64%   7.86 ± 1.20   7.23 ± 1.53   M.L. Nguyen and S. Horiguchi, “A new sentence
    SVMP-R      58.31%   8.25 ± 1.30   7.54 ± 1.39      reduction based on Decision tree model”,
    SVMP-10     57.62%   8.60 ± 1.32   7.71 ± 1.41      Proceedings of 17th Pacific Asia Conference
    Human       64.00%   9.01 ± 0.25   8.40 ± 0.60      on Language, Information and Computation,
                                                        pp. 290-297, 2003
                                                     V. Vapnik, “The Natural of Statistical Learning
                                                        Theory”, New York: Springer-Verlag, 1995.
6     Conclusions                                    J. Platt,“ Probabilistic outputs for support vec-
We have presented a new probabilistic sentence          tor machines and comparison to regularized
reduction approach that enables a long sentence         likelihood methods,” in Advances in Large
to be rewritten into reduced sentences based on         Margin Classifiers, Cambridege, MA: MIT
support vector models. Our methods achieves             Press, 2000.
better performance when compared with earlier        B. Scholkopf et al, “Comparing Support Vec-
methods. The proposed reduction approach can            tor Machines with Gausian Kernels to Radius
generate multiple best outputs. Experimental            Basis Function Classifers”, IEEE Trans. Sig-
results showed that the top 10 reduced sentences        nal Procesing, 45, pp. 2758-2765, 1997.
returned by the reduction process might yield
accuracies higher than previous work. We be-
lieve that a good ranking method might improve
the sentence reduction performance further in a
text.
