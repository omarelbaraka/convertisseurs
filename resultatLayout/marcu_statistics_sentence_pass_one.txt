From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved.




   Statistics-Based Summarization — Step One: Sentence Compression
                                           Kevin Knight and Daniel Marcu
                            Information Sciences Institute and Department of Computer Science
                                             University of Southern California
                                             4676 Admiralty Way, Suite 1001
                                                Marina del Rey, CA 90292
                                                  {knight,marcu}@isi.edu


                           Abstract                                      rules for deleting information that is redundant, com-
                                                                         pressing long sentences into shorter ones, aggregating
   When humans produce summaries of documents, they
                                                                         sentences, repairing reference links, etc.
   do not simply extract sentences and concatenate them.
   Rather, they create new sentences that are grammati-                     Our goal is also to generate coherent abstracts. How-
   cal, that cohere with one another, and that capture the               ever, in contrast with the above work, we intend to
   most salient pieces of information in the original doc-               eventually use Abstract, Text tuples, which are widely
   ument. Given that large collections of text/abstract                  available, in order to automatically learn how to rewrite
   pairs are available online, it is now possible to envision            Texts as coherent Abstracts. In the spirit of the work
   algorithms that are trained to mimic this process. In                 in the statistical MT community, which is focused on
   this paper, we focus on sentence compression, a sim-                  sentence-to-sentence translations, we also decided to fo-
   pler version of this larger challenge. We aim to achieve              cus ﬁrst on a simpler problem, that of sentence compres-
   two goals simultaneously: our compressions should be                  sion. We chose this problem for two reasons:
   grammatical, and they should retain the most impor-
   tant pieces of information. These two goals can con-                  • First, the problem is complex enough to require the
   ﬂict. We devise both noisy-channel and decision-tree                    development of sophisticated compression models:
   approaches to the problem, and we evaluate results                      Determining what is important in a sentence and
   against manual compressions and a simple baseline.                      determining how to convey the important informa-
                                                                           tion grammatically, using only a few words, is just a
                       Introduction                                        scaled down version of the text summarization prob-
 Most of the research in automatic summarization                           lem. Yet, the problem is simple enough, since we do
 has focused on extraction, i.e., on identifying the                       not have to worry yet about discourse related issues,
 most important clauses/sentences/paragraphs in texts                      such as coherence, anaphors, etc.
 (see (Mani & Maybury 1999) for a representative col-                    • Second, an adequate solution to this problem has
 lection of papers). However, determining the most im-                     an immediate impact on several applications. For
 portant textual segments is only half of what a summa-                    example, due to time and space constraints, the
 rization system needs to do because, in most cases, the                   generation of TV captions often requires only the
 simple catenation of textual segments does not yield                      most important parts of sentences to be shown on a
 coherent outputs. Recently, a number of researchers                       screen (Linke-Ellis 1999; Robert-Ribes et al. 1999).
 have started to address the problem of generating co-                     A good sentence compression module would there-
 herent summaries: McKeown et al. (1999), Barzilay et                      fore have an impact on the task of automatic cap-
 al. (1999), and Jing and McKeown (1999) in the context                    tion generation. A sentence compression module
 of multidocument summarization; Mani et al. (1999) in                     can also be used to provide audio scanning ser-
 the context of revising single document extracts; and                     vices for the blind (Grefenstette 1998). In gen-
 Witbrock and Mittal (1999) in the context of headline                     eral, since all systems aimed at producing coher-
 generation.                                                               ent abstracts implement manually written sets of
    The approach proposed by Witbrock and Mit-                             sentence compression rules (McKeown et al. 1999;
 tal (1999) is the only one that applies a probabilistic                   Mani, Gates, & Bloedorn 1999; Barzilay, McKeown,
 model trained directly on Headline, Document pairs.                     & Elhadad 1999), it is likely that a good sentence
 However, this model has yet to scale up to generat-                       compression module would impact the overall quality
 ing multiple-sentence abstracts as well as well-formed,                   of these systems as well. This becomes particularly
 grammatical sentences. All other approaches employ                        important for text genres that use long sentences.
 sets of manually written or semi-automatically derived
                                                                           In this paper, we present two approaches to the sen-
 Copyright  c 2000, American Association for Artiﬁcial In-              tence compression problem. Both take as input a se-
 telligence (www.aaai.org). All rights reserved.                         quence of words W = w1 , w2 , . . . , wn (one sentence).
An algorithm may drop any subset of these words. The             It is advantageous to break the problem down this
words that remain (order unchanged) form a compres-           way, as it decouples the somewhat independent goals
sion. There are 2n compressions to choose from—some           of creating a short text that (1) looks grammatical,
are reasonable, most are not. Our ﬁrst approach de-           and (2) preserves important information. It is easier to
velops a probabilistic noisy-channel model for sentence       build a channel model that focuses exclusively on the
compression. The second approach develops a decision-         latter, without having to worry about the former. That
based, deterministic model.                                   is, we can specify that a certain substring may represent
                                                              unimportant information, but we do not need to worry
   A noisy-channel model for sentence                         that deleting it will result in an ungrammatical struc-
              compression                                     ture. We leave that to the source model, which worries
                                                              exclusively about well-formedness. In fact, we can make
This section describes a probabilistic approach to the        use of extensive prior work in source language modeling
compression problem. In particular, we adopt the noisy        for speech recognition, machine translation, and natu-
channel framework that has been relatively successful in      ral language generation. The same goes for actual com-
a number of other NLP applications, including speech          pression (“decoding” in noisy-channel jargon)—we can
recognition (Jelinek 1997), machine translation (Brown        re-use generic software packages to solve problems in all
et al. 1993), part-of-speech tagging (Church 1988),           these application domains.
transliteration (Knight & Graehl 1998), and informa-
tion retrieval (Berger & Laﬀerty 1999).                       Statistical Models
   In this framework, we look at a long string and imag-
ine that (1) it was originally a short string, and then       In the experiments we report here, we build very sim-
(2) someone added some additional, optional text to it.       ple source and channel models. In a departure from
Compression is a matter of identifying the original short     the above discussion and from previous work on statis-
string. It is not critical whether or not the “original”      tical channel models, we assign probabilities Ptree (s)
string is real or hypothetical. For example, in statistical   and Pexpand tree (t | s) to trees rather than strings. In
machine translation, we look at a French string and say,      decoding a new string, we ﬁrst parse it into a large tree t
“This was originally English, but someone added ‘noise’       (using Collins’ parser (1997)), and we then hypothesize
to it.” The French may or may not have been translated        and rank various small trees.
from English originally, but by removing the noise, we           Good source strings are ones that have both (1) a
can hypothesize an English source—and thereby trans-          normal-looking parse tree, and (2) normal-looking word
late the string. In the case of compression, the noise        pairs. Ptree (s) is a combination of a standard proba-
consists of optional text material that pads out the core     bilistic context-free grammar (PCFG) score, which is
signal. For the larger case of text summarization, it may     computed over the grammar rules that yielded the tree
be useful to imagine a scenario in which a news editor        s, and a standard word-bigram score, which is com-
composes a short document, hands it to a reporter, and        puted over the leaves of the tree. For example, the
tells the reporter to “ﬂesh it out” . . . which results in    tree s =(S (NP John) (VP (VB saw) (NP Mary))) is
the article we read in the newspaper. As summarizers,         assigned a score based on these factors:
we may not have access to the editor’s original version
(which may or may not exist), but we can guess at it—            Ptree (s) = P(TOP → S | TOP) ·
which is where probabilities come in.                            P(S → NP VP | S) · P(NP → John | NP) ·
   As in any noisy channel application, we must solve            P(VP → VB NP | VP) · P(VP → saw | VB) ·
three problems:                                                  P(NP → Mary | NP) ·
                                                                 P(John | EOS) · P(saw | John) ·
• Source model. We must assign to every string s a               P(Mary | saw) · P(EOS | Mary)
  probability P(s), which gives the chance that s is gen-
  erated as an “original short string” in the above hy-          Our stochastic channel model performs minimal op-
  pothetical process. For example, we may want P(s)           erations on a small tree s to create a larger tree t. For
  to be very low if s is ungrammatical.                       each internal node in s, we probabilistically choose an
                                                              expansion template based on the labels of the node and
• Channel model. We assign to every pair of strings           its children. For example, when processing the S node
  s, t a probability P(t | s), which gives the chance       in the tree above, we may wish to add a prepositional
  that when the short string s is expanded, the result        phrase as a third child. We do this with probability
  is the long string t. For example, if t is the same         P(S → NP VP PP | S → NP VP). Or we may choose
  as s except for the extra word “not,” then we may           to leave it alone, with probability P(S → NP VP | S →
  want P(t | s) to be very low. The word “not” is not         NP VP). After we choose an expansion template, then
  optional, additional material.                              for each new child node introduced (if any), we grow a
• Decoder. When we observe a long string t, we search         new subtree rooted at that node—for example (PP (P
  for the short string s that maximizes P(s | t). This        in) (NP Pittsburgh)). Any particular subtree is grown
  is equivalent to searching for the s that maximizes         with probability given by its PCFG factorization, as
  P(s) · P (t | s).                                           above (no bigrams).
              G                          G                           G                The documentation is typical of Epson quality: excellent.
         H               A           H           A
                                                              F           D           Documentation is excellent.
         a     C         B       D   a       C       D                    e
               b Q           R   e
                                                          H         K                 All of our design goals were achieved and the delivered
                                             b       e              b
                   Z         d
                                                          a                           performance matches the speed of the underlying device.
                   c
                                                                                      All design goals were achieved.
                   (t)                   (s1)                     (s2)                Reach’s E-mail product, MailMan, is a message- manage-
                                                                                      ment system designed initially for VINES LANs that will
             Figure 1: Examples of parse trees.                                       eventually be operating system-independent.
                                                                                      MailMan will eventually be operating system-independent.
                                                                                      Although the modules themselves may be physically and/or
                                                                                      electrically incompatible, the cable-speciﬁc jacks on them
Example                                                                               provide industry-standard connections.
                                                                                      Cable-speciﬁc jacks provide industry-standard connections.
In this section, we show how to tell whether one poten-                               Ingres/Star prices start at $2,100.
tial compression is more likely than another, according                               Ingres/Star prices start at $2,100.
to the statistical models described above. Suppose we
observe the tree t in Figure 1, which spans the string
abcde. Consider the compression s1, which is shown in                                      Figure 2: Examples from our parallel corpus.
the same ﬁgure.
  We     compute       the factors Ptree (s1) and                                         P(G → H A | G → H A)
Pexpand tree (t | s1). Breaking this down further,                                        P(A → C B D | A → C B D)
the source PCFG and word-bigram factors, which
                                                                                           P(B → Q R | B → Q R)
describe Ptree (s1), are:
                                                                                           P(Q → Z | Q → Z)
   P(TOP → G | TOP)                       P(H → a | H)
   P(G → H A | G)                         P(C → b | C)                                   Now we can simply compare Pexpand tree (s1 |
   P(A → C D | A)                         P(D → e | D)                                t) = Ptree (s1) · Pexpand tree (t | s1))/Ptree (t) ver-
                                                                                      sus Pexpand tree (t | t) = Ptree (t) · Pexpand tree (t |
   P(a | EOS)                             P(e | b)
   P(b | a)                               P(EOS | e)                                  t))/Ptree (t) and select the more likely one. Note that
                                                                                      Ptree (t) and all the PCFG factors can be canceled out,
                                                                                      as they appear in any potential compression. Therefore,
The channel expansion-template factors and the chan-                                  we need only compare compressions of the basis of the
nel PCFG (new tree growth) factors, which describe                                    expansion-template probabilities and the word-bigram
Pexpand tree (t | s1), are:                                                           probabilities. The quantities that diﬀer between the
                                                                                      two proposed compressions are boxed above. There-
   P(G → H A | G → H A)                                                               fore, s1 will be preferred over t if and only if:
   P(A → C B D | A → C D)                                                                 P(e | b) · P(A → C B D | A → C D) >
   P(B → Q R | B)                                        P(Z → c | Z)                     P(b | a) · P(c | b) · P(d | c) ·
   P(Q → Z | Q)                                          P(R → d | R)                     P(A → C B D | A → C B D) ·
                                                                                          P(B → Q R | B → Q R) · P(Q → Z | Q → Z)
   A diﬀerent compression will be scored with a diﬀerent
set of factors. For example, consider a compression of                                Training Corpus
t that leaves t completely untouched. In that case, the                               In order to train our system, we used the Ziﬀ-Davis
source costs Ptree (t) are:                                                           corpus, a collection of newspaper articles announcing
                                                                                      computer products. Many of the articles in the corpus
   P(TOP → G | TOP)                       P(H → a | H)                   P(a | EOS)   are paired with human written abstracts. We automat-
   P(G → H A | G)                         P(C → b | C)                   P(b | a)     ically extracted from the corpus a set of 1067 sentence
                                                                                      pairs. Each pair consisted of a sentence t = t1 , t2 , . . . , tn
   P(A → C D | A)                         P(Z → c | Z)                   P(c | b)     that occurred in the article and a possibly compressed
   P(B → Q R | B)                         P(R → d | R)                   P(d | c)     version of it s = s1 , s2 , . . . , sm , which occurred in the
   P(Q → Z | Q)                           P(D → e | D)                   P(e | d)     human written abstract. Figure 2 shows a few sentence
                                                                         P(EOS | e)   pairs extracted from the corpus.
                                                                                         We decided to use such a corpus because it is con-
                                                                                      sistent with two desiderata speciﬁc to summarization
The channel costs Pexpand tree (t | t) are:                                           work: (i) the human-written Abstract sentences are
grammatical; (ii) the Abstract sentences represent in a     semantic representation into a vast number of potential
compressed form the salient points of the original news-    English renderings. These renderings are packed into
paper Sentences. We decided to keep in the corpus un-       a forest, from which the most promising sentences are
compressed sentences as well, since we want to learn        extracted using statistical scoring.
not only how to compress a sentence, but also when to         For our purposes, the extractor selects the trees with
do it.                                                      the best combination of word-bigram and expansion-
                                                            template scores. It returns a list of such trees, one for
Learning Model Parameters                                   each possible compression length. For example, for
We collect expansion-template probabilities from our        the sentence Beyond that basic level, the operations of
parallel corpus. We ﬁrst parse both sides of the parallel   the three products vary, we obtain the following “best”
corpus, and then we identify corresponding syntactic        compressions, with negative log-probabilities shown in
nodes. For example, the parse tree for one sentence         parentheses (smaller = more likely):
may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while
the parse tree for its compressed version may begin (S      Beyond that basic level, the operations of the three products vary
(NP . . . ) (VP . . . )). If these two S nodes are deemed   widely (1514588)
to correspond, then we chalk up one joint event (S →        Beyond that level, the operations of the three products vary widely
NP VP, S → NP VP PP); afterwards we normalize.              (1430374)
Not all nodes have corresponding partners; some non-        Beyond that basic level, the operations of the three products vary
correspondences are due to incorrect parses, while oth-     (1333437)
ers are due to legitimate reformulations that are beyond    Beyond that level, the operations of the three products vary
the scope of our simple channel model. We use standard      (1249223)
methods to estimate word-bigram probabilities.              Beyond that basic level, the operations of the products vary
                                                            (1181377)
Decoding                                                    The operations of the three products vary widely (939912)
There is a vast number of potential compressions of a       The operations of the products vary widely (872066)
large tree t, but we can pack them all eﬃciently into a     The operations of the products vary (748761)
shared-forest structure. For each node of t that has n      The operations of products vary (690915)
children, we                                                Operations of products vary (809158)
• generate 2n − 1 new nodes, one for each non-empty         The operations vary (522402)
   subset of the children, and                              Operations vary (662642)

• pack those nodes so that they are referred to as a        Length Selection
   whole.
                                                            It is useful to have multiple answers to choose from, as
For example, consider the large tree t above. All com-
                                                            one user may seek a 20% compression, while another
pressions can be represented with the following forest:
                                                            seeks a 60% compression. However, for purposes of
   G→HA         B→R             A→BC         H→a            evaluation, we want our system to be able to select a
   G→H          Q→Z             A→C          C→b            single compression. If we rely on the log-probabilities
   G→A          A→CBD           A→B          Z→c            as shown above, we will almost always choose the short-
   B→QR         A→CB            A→D          R→d            est compression. (Note above, however, how the three-
   B→Q          A→CD                         D→e            word compression scores better than the two-word com-
                                                            pression, as the models are not entirely happy removing
   We can also assign an expansion-template probability     the article “the”). To create a more fair competition,
to each node in the forest. For example, to the B →         we divide the log-probability by the length of the com-
Q node, we can assign P(B → Q R | B → Q). If the            pression, rewarding longer strings. This is commonly
observed probability from the parallel corpus is zero,      done in speech recognition.
then we assign a small ﬂoor value of 10−6 . In reality,        If we plot this normalized score against compression
we produce forests that are much slimmer, as we only        length, we usually observe a (bumpy) U-shaped curve,
consider compressing a node in ways that are locally        as illustrated in Figure 3. In a typical more diﬃcult
grammatical according to the Penn Treebank—if a rule        case, a 25-word sentence may be optimally compressed
of the type A → C B has never been observed, then it        by a 17-word version. Of course, if a user requires a
will not appear in the forest.                              shorter compression than that, she may select another
   At this point, we want to extract a set of high-         region of the curve and look for a local minimum.
scoring trees from the forest, taking into account
both expansion-template probabilities and word-bigram          A decision-based model for sentence
probabilities. Fortunately, we have such a generic ex-
tractor on hand (Langkilde 2000). This extractor was                       compression
designed for a hybrid symbolic-statistical natural lan-     In this section, we describe a decision-based, history
guage generation system called Nitrogen. In that ap-        model of sentence compression. As in the noisy-channel
plication, a rule-based component converts an abstract      approach, we again assume that we are given as input
                                                                                                                                                                                                                                                                                                                              Stack       Input List                                                Stack           Input List




                                                                                                                                                                                                          Finally another advantage of broadband is distance .
                                                                                                                                                                                                                                                                 Finally, another advantage of broadband is distance .
                                                                                                                                                                                                                                                                                                                                                                                                        F           B        D
                                                                                                                                                                                                                                                                                                                                          G                                                                         Q   R    e   DROP B
                                                                                                                                                                                                                                                                                                                                                                                                H           K
                                                                                                                                                                                                                                                                                                                                          H    A                                                                    Z   d
                                                                                                                                                                                                                                                                                                                                          a     C          B       D                            a           b       c
                                                                                                                                                                                                                                                                                                                                                                       SHIFT;                                                             STEP 6
                                                                                                                                                                                                                                                                                                                                                b          Q   R   e
                                                                                                                                                                                                                                                                                                                                                                       ASSIGNTYPE H                         F       D
                                                                                                                                                                                                                                                                                                                                                           Z   d
   Adjusted negative log-probability of best




                                                      Advantage is distance .
                                                                                                                                                                                                                                                                                                                                                           c                    STEPS 1-2                           e            SHIFT;
                                                                                                                                                                                                                                                                                                                                                                                                    H           K
                                                                                                                                                                                                                                                                                                                                                                                                                                 ASSIGNTYPE D




                                                                                                                                                           Another advantage of broadband is distance .
   compression s at a particular length n


                                                                                                                                                                                                                                                                                                                                      H   A




                                                                                Another advantage is distance .
                                                                                                                                                                                                                                                                                                                                                                                                    a           b                       STEPS 7-8
                                                                                                                                                                                                                                                                                                                                          C    B               D
                                                                                                                                                                                                                                                                                                                                      a                                SHIFT;




                                                                                                                    Advantage of broadband is distance .
                                                                                                                                                                                                                                                                                                                                          b    Q       R       e                                F               D
                                                                                                                                                                                                                                                                                                                                               Z       d               ASSIGNTYPE K
                                                                                                                                                                                                                                                                                                                                                                                            H       K           e                REDUCE 2 G
                                                                                                                                                                                                                                                                                                                                               c                                STEPS 3-4
                                                                                                                                                                                                                                                                                                                                                                                            a
           -log P(s) P( t | s) / n




                                               0.20                                                                                                                                                                                                                                                                           H       K   B            D
                                                                                                                                                                                                                                                                                                                                                                                                    b                                     STEP 9

                                                                                                                                                                                                                                                                                                                                                       e                                                G
                                                                                                                                                                                                                                                                                                                                          Q   R
                                                                                                                                                                                                                                                                                                                              a       b                                REDUCE 2 F
                                                                                                                                                                                                                                                                                                                                          Z   d                                                 F               D
                                                                                                                                                                                                                                                                                                                                          c                                     STEP 5
                                                                                                                                                                                                                                                                                                                                                                                            H       K           e
                                                                                                                                                                                                                                                                                                                                                                                            a       b




                                                                                                                                                                                                                                                                                                                          Figure 4: Example of incremental tree compression.
                                               0.15


                                                                                                                                                                                                                                                                                                                           input list all words that are spanned by constituent
                                                                                                                                                                                                                                                                                                                           x in t.
                                                                                                                                                                                                                                                                                                                         • assignType operations are used to change the label
                                               0.10                                                                                                                                                                                                                                                                        of trees at the top of the stack. These actions assign
                                                                                                                                                                                                                                                                                                                           POS tags to the words in the compressed sentence,
                                                                                                                                                                                                                                                                                                                           which may be diﬀerent from the POS tags in the
                                                          4                             5                           6                                          7                                          8                                                          9
                                                                                                                                                                                                                                                                                                                           original sentence.

                                                                                                                  Compression length n                                                                                                                                                                                   The decision-based model is more ﬂexible than the
                                                                                                                                                                                                                                                                                                                         channel model because it enables the derivation of trees
                                                                                                                                                                                                                                                                                                                         whose skeleton can diﬀer quite drastically from that of
Figure 3: Adjusted log-probabilities for top-scoring                                                                                                                                                                                                                                                                     the tree given as input. For example, using the channel
compressions at various lengths (lower is better).                                                                                                                                                                                                                                                                       model, we are unable to obtain tree s2 from t. However,
                                                                                                                                                                                                                                                                                                                         the four operations listed above enable us to rewrite a
                                                                                                                                                                                                                                                                                                                         tree t into any tree s, as long as an in-order traversal of
a parse tree t. Our goal is to “rewrite” t into a smaller                                                                                                                                                                                                                                                                the leaves of s produces a sequence of words that occur
tree s, which corresponds to a compressed version of the                                                                                                                                                                                                                                                                 in the same order as the words in the tree t. For exam-
original sentence subsumed by t. Suppose we observe in                                                                                                                                                                                                                                                                   ple, the tree s2 can be obtained from tree t by following
our corpus the trees t and s2 in Figure 1. In this model,                                                                                                                                                                                                                                                                this sequence of actions, whose eﬀects are shown in Fig-
we ask ourselves how we may go about rewriting t into                                                                                                                                                                                                                                                                    ure 4: shift; assignType H; shift; assignType K;
s2. One possible solution is to decompose the rewriting                                                                                                                                                                                                                                                                  reduce 2 F; drop B; shift; assignType D; reduce
operation into a sequence of shift-reduce-drop actions                                                                                                                                                                                                                                                                   2 G.
that are speciﬁc to an extended shift-reduce parsing
                                                                                                                                                                                                                                                                                                                            To save space, we show shift and assignType op-
paradigm.
                                                                                                                                                                                                                                                                                                                         erations on the same line; however, the reader should
   In the model we propose, the rewriting process starts
                                                                                                                                                                                                                                                                                                                         understand that they correspond to two distinct ac-
with an empty Stack and an Input List that contains the
                                                                                                                                                                                                                                                                                                                         tions. As one can see, the assignType K operation
sequence of words subsumed by the large tree t. Each
                                                                                                                                                                                                                                                                                                                         rewrites the POS tag of the word b; the reduce op-
word in the input list is labeled with the name of all syn-
                                                                                                                                                                                                                                                                                                                         erations modify the skeleton of the tree given as input.
tactic constituents in t that start with it (see Figure 4).
                                                                                                                                                                                                                                                                                                                         To increase readability, the input list is shown in a for-
At each step, the rewriting module applies an opera-
                                                                                                                                                                                                                                                                                                                         mat that resembles as closely as possible the graphical
tion that is aimed at reconstructing the smaller tree s2.
                                                                                                                                                                                                                                                                                                                         representation of the trees in ﬁgure 1.
In the context of our sentence-compression module, we
need four types of operations:
                                                                                                                                                                                                                                                                                                                         Learning the parameters of the
• shift operations transfer the ﬁrst word from the in-                                                                                                                                                                                                                                                                   decision-based model
  put list into the stack;
                                                                                                                                                                                                                                                                                                                         We associate with each conﬁguration of our shift-
• reduce operations pop the k syntactic trees located                                                                                                                                                                                                                                                                    reduce-drop, rewriting model a learning case. The cases
  at the top of the stack; combine them into a new                                                                                                                                                                                                                                                                       are generated automatically by a program that derives
  tree; and push the new tree on the top of the stack.                                                                                                                                                                                                                                                                   sequences of actions that map each of the large trees in
  Reduce operations are used to derive the structure of                                                                                                                                                                                                                                                                  our corpus into smaller trees. The rewriting procedure
  the syntactic tree of the short sentence.                                                                                                                                                                                                                                                                              simulates a bottom-up reconstruction of the smaller
• drop operations are used to delete from the input list                                                                                                                                                                                                                                                                 trees.
  subsequences of words that correspond to syntactic                                                                                                                                                                                                                                                                       Overall, the 1067 pairs of long and short sentences
  constituents. A drop x operations deletes from the                                                                                                                                                                                                                                                                     yielded 46383 learning cases. Each case was labeled
with one action name from a set of 210 possible ac-                              Evaluation
tions: There are 37 distinct assignType actions, one        To evaluate our compression algorithms, we randomly
for each POS tag. There are 63 distinct drop actions,       selected 32 sentence pairs from our parallel corpus,
one for each type of syntactic constituent that can be      which we will refer to as the Test Corpus. We used the
deleted during compression. There are 109 distinct re-      other 1035 sentence pairs for training. Figure 5 shows
duce actions, one for each type of reduce operation that    three sentences from the Test Corpus, together with the
is applied during the reconstruction of the compressed      compressions produced by humans, our compression al-
sentence. And there is one shift operation. Given a         gorithms, and a baseline algorithm that produces com-
tree t and an arbitrary conﬁguration of the stack and       pressions with highest word-bigram scores. The exam-
input list, the purpose of the decision-based classiﬁer     ples are chosen so as to reﬂect good, average, and bad
is to learn what action to choose from the set of 210       performance cases. The ﬁrst sentence is compressed in
possible actions.                                           the same manner by humans and our algorithms (the
   To each learning example, we associated a set of 99      baseline algorithm chooses though not to compress this
features from the following two classes:                    sentence). For the second example, the output of the
                                                            Decision-based algorithm is grammatical, but the se-
Operational features reﬂect the number of trees             mantics is negatively aﬀected. The noisy-channel al-
 in the stack, the input list, and the types of             gorithm deletes only the word “break”, which aﬀects
 the last ﬁve operations. They also encode infor-           the correctness of the output less. In the last example,
 mation that denote the syntactic category of the           the noisy-channel model is again more conservative and
 root nodes of the partial trees built up to a cer-         decides not to drop any constituents. In constrast, the
 tain time. Examples of such features are: num-             decision-based algorithm compresses the input substan-
 berTreesInStack, wasPreviousOperationShift, syn-           tially, but it fails to produce a grammatical output.
 tacticLabelOfTreeAtTheTopOfStack, etc.                        We presented each original sentence in the Test Cor-
Original-tree-speciﬁc features denote the syntac-           pus to four judges, together with four compressions of it:
 tic constituents that start with the ﬁrst unit in the      the human generated compression, the outputs of the
 input list. Examples of such features are: inputList-      noisy-channel and decision-based algorithms, and the
 StartsWithA CC, inputListStartsWithA PP, etc.              output of the baseline algorithm. The judges were told
                                                            that all outputs were generated automatically. The or-
   The decision-based compression module uses the           der of the outputs was scrambled randomly across test
C4.5 program (Quinlan 1993) in order to learn deci-         cases.
sion trees that specify how large syntactic trees can          To avoid confounding, the judges participated in two
be compressed into shorter trees. A ten-fold cross-         experiments. In the ﬁrst experiment, they were asked
validation evaluation of the classiﬁer yielded an accu-     to determine on a scale from 1 to 5 how well the systems
racy of 87.16% (± 0.14). A majority baseline classi-        did with respect to selecting the most important words
ﬁer that chooses the action shift has an accuracy of        in the original sentence. In the second experiment, they
28.72%.                                                     were asked to determine on a scale from 1 to 5 how
                                                            grammatical the outputs were.
Employing the decision-based model                             We also investigated how sensitive our algorithms are
                                                            with respect to the training data by carrying out the
To compress sentences, we apply the shift-reduce-drop       same experiments on sentences of a diﬀerent genre, the
model in a deterministic fashion. We parse the sentence     scientiﬁc one. To this end, we took the ﬁrst sentence of
to be compressed (Collins 1997) and we initialize the       the ﬁrst 26 articles made available in 1999 on the cmplg
input list with the words in the sentence and the syn-      archive. We created a second parallel corpus, which
tactic constituents that “begin” at each word, as shown     we will refer to as the Cmplg Corpus, by generating
in Figure 4. We then incrementally inquire the learned      by ourselves compressed grammatical versions of these
classiﬁer what action to perform, and we simulate the       sentences. Since some of the sentences in this corpus
execution of that action. The procedure ends when the       were extremely long, the baseline algorithm could not
input list is empty and when the stack contains only        produce compressed versions in reasonable time.
one tree. An inorder traversal of the leaves of this tree      The results in Table 1 show compression rates, and
produces the compressed version of the sentence given       mean and standard deviation results across all judges,
as input.                                                   for each algorithm and corpus. The results show that
   Since the model is deterministic, it produces only one   the decision-based algorithm is the most aggressive:
output. The advantage is that the compression is very       on average, it compresses sentences to about half of
fast: it takes only a few milliseconds per sentence. The    their original size. The compressed sentences produced
disadvantage is that it does not produce a range of         by both algorithms are more “grammatical” and con-
compressions, from which another system may subse-          tain more important words than the sentences pro-
quently choose. It is straightforward though to extend      duced by the baseline. T -test experiments showed these
the model within a probabilistic framework by applying,     diﬀerences to be statistically signiﬁcant at p < 0.01
for example, the techniques used by Magerman (1995).        both for individual judges and for average scores across
Original:       Beyond the basic level, the operations of the three products vary widely.
Baseline:       Beyond the basic level, the operations of the three products vary widely.
Noisy-channel: The operations of the three products vary widely.
Decision-based: The operations of the three products vary widely.
Humans:         The operations of the three products vary widely.

Original:       Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles.
Baseline:       Arborscan and worked in, but it very large dxf.
Noisy-channel: Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles.
Decision-based: Arborscan is reliable and worked accurately in testing very large dxf ﬁles.
Humans:         Arborscan produces very large dxf ﬁles.

Original:       Many debugging features, including user-deﬁned break points and variable-watching and
                message-watching windows, have been added.
Baseline:       Debugging, user-deﬁned and variable-watching and message-watching, have been.
Noisy-channel: Many debugging features, including user-deﬁned points and variable-watching and
                message-watching windows, have been added.
Decision-based: Many debugging features.
Humans:         Many debugging features have been added .

                                             Figure 5: Compression examples

      Corpus    Avg. orig. sent. length                       Baseline    Noisy-channel     Decision-based      Humans
      Test            21 words            Compression         63.70%         70.37%            57.19%            53.33%
                                          Grammaticality     1.78±1.19     4.34±1.02          4.30±1.33        4.92±0.18
                                          Importance         2.17±0.89     3.38±0.67          3.54±1.00        4.24 ±0.52
      Cmplg            26 words           Compression            –           65.68%            54.25%            65.68%
                                          Grammaticality         –         4.22±0.99          3.72±1.53        4.97±0.08
                                          Importance             –         3.42±0.97          3.24±0.68        4.32±0.54

                                              Table 1: Experimental results


all judges. T -tests showed no signiﬁcant statistical                                   References
diﬀerences between the two algorithms. As Table 1
shows, the performance of the compression algorithms               Barzilay, R.; McKeown, K.; and Elhadad, M. 1999.
is much closer to human performance than baseline per-             Information fusion in the context of multi-document
formance; yet, humans perform statistically better than            summarization. In Proceedings of the 37th Annual
our algorithms at p < 0.01.                                        Meeting of the Association for Computational Linguis-
                                                                   tics (ACL–99), 550–557.
   When applied to sentences of a diﬀerent genre, the              Berger, A., and Laﬀerty, J. 1999. Information retrieval
performance of the noisy-channel compression algo-                 as statistical translation. In Proceedings of the 22nd
rithm degrades smoothly, while the performance of the              Conference on Research and Development in Informa-
decision-based algorithm drops sharply. This is due to             tion Retrieval (SIGIR–99), 222–229.
a few sentences in the Cmplg Corpus that the decision-
based algorithm over-compressed to only two or three               Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mer-
words. We suspect that this problem can be ﬁxed if                 cer, R. 1993. The mathematics of statistical ma-
the decision-based compression module is extended in               chine translation: Parameter estimation. Computa-
the style of Magerman (1995), by computing probabil-               tional Linguistics 19(2):263–311.
ities across the sequences of decisions that correspond            Church, K. 1988. A stochastic parts program and noun
to a compressed sentence. Likewise, there are substan-             phrase parser for unrestricted text. In Proceedings of
tial gains to be had in noisy-channel modeling—we see              the Second Conference on Applied Natural Language
clearly in the data many statistical dependencies and              Processing, 136–143.
processes that are not captured in our simple initial
models. More grammatical output will come from tak-                Collins, M. 1997. Three generative, lexicalized mod-
ing account of subcategory and head-modiﬁer statistics             els for statistical parsing. In Proceedings of the 35th
(in addition to simple word-bigrams), and an expanded              Annual Meeting of the Association for Computational
channel model will allow for more tree manipulation                Linguistics (ACL–97), 16–23.
possibilities. Work on extending the algorithms pre-               Grefenstette, G. 1998. Producing intelligent tele-
sented in this paper to compressing multiple sentences             graphic text reduction to provide an audio scanning
is currently underway.                                             service for the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text Summarization,
111–118.
Jelinek, F. 1997. Statistical Methods for Speech Recog-
nition. The MIT Press.
Jing, H., and McKeown, K. 1999. The decomposition
of human-written summary sentences. In Proceedings
of the 22nd Conference on Research and Development
in Information Retrieval (SIGIR–99).
Knight, K., and Graehl, J. 1998. Machine transliter-
ation. Computational Linguistics 24(4):599–612.
Langkilde, I. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics.
Linke-Ellis, N. 1999. Closed captioning in Amer-
ica: Looking beyond compliance. In Proceedings of
the TAO Workshop on TV Closed Captions for the
hearing impaired people, 43–59.
Magerman, D. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd Annual Meeting
of the Association for Computational Linguistics, 276–
283.
Mani, I., and Maybury, M., eds. 1999. Advances in
Automatic Text Summarization. The MIT Press.
Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improving
summaries by revising them. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, 558–565.
McKeown, K.; Klavans, J.; Hatzivassiloglou, V.;
Barzilay, R.; and Eskin, E. 1999. Towards multidoc-
ument summarization by reformulation: Progress and
prospects. In Proceedings of the Sixteenth National
Conference on Artiﬁcial Intelligence (AAAI–99).
Quinlan, J. 1993. C4.5: Programs for Machine Learn-
ing. San Mateo, CA: Morgan Kaufmann Publishers.
Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burn-
ham, D. 1999. Semi-automatic captioning of TV pro-
grams, an Australian perspective. In Proceedings of
the TAO Workshop on TV Closed Captions for the
hearing impaired people, 87–100.
Witbrock, M., and Mittal, V.            1999.    Ultra-
summarization: A statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd International Conference on Re-
search and Development in Information Retrieval (SI-
GIR’99), Poster Session, 315–316.
