                             AUTOMATIC LINGUISTIC SEGMENTATION
                                 OF CONVERSATIONAL SPEECH
                                           Andreas Stolcke                   Elizabeth Shriberg

                                           Speech Technology and Research Laboratory
                                             SRI International, Menlo Park, CA 94025
                                          stolcke@speech.sri.com ees@speech.sri.com



                         ABSTRACT                                         could use the knowledge incorporated in an automatic segmenter
                                                                          to help end-point a user’s speech input. A speech indexing and re-
As speech recognition moves toward more unconstrained domains
                                                                          trieval system (such as for transcribed broadcast audio) could pro-
such as conversational speech, we encounter a need to be able to
                                                                          cess its data in more meaningful units if the locations of linguistic
segment (or resegment) waveforms and recognizer output into lin-
                                                                          segment boundaries were known.
guistically meaningful units, such a sentences. Toward this end,
we present a simple automatic segmenter of transcripts based on           Our main motivation for the work reported here comes from speech
N-gram language modeling. We also study the relevance of sev-             language modeling. Experiments at the 1995 Johns Hopkins Lan-
eral word-level features for segmentation performance. Using only         guage Modeling Workshop showed that the quality of a language
word-level information, we achieve 85% recall and 70% precision           model (LM) can be improved if both training and test data are seg-
on linguistic boundary detection.                                         mented linguistically, rather than acoustically [8]. We showed in
                                                                          [10] and [9] that proper modeling of filled pauses requires knowl-
                  1.    INTRODUCTION                                      edge of linguistic segment boundaries. We found for example that
Today’s large-vocabulary speech recognizers typically prefer to pro-      segment-internal filled pauses condition the following words quite
cess a few tens of seconds of speech at a time, to keep the time and      differently from segment-initial filled pauses. Finally, recent efforts
memory demands of the decoder within bounds. For longer inputs,           in language modeling for conversational speech, such as [8], attempt
the waveform is usually presegmented into shorter pieces based on         to capitalize on the internal structure of utterances and turns. Such
simple acoustic criteria, such as nonspeech intervals (e.g., pauses)      models are formulated in terms of linguistic units and therefore re-
and turn boundaries (when several speakers are involved). We refer        quire linguistic segmentations to be applicable.
to such segmentations as acoustic segmentations.
                                                                                                  3.    METHOD
Acoustic segmentations generally do not reflect the linguistic struc-
                                                                          Our main goal for this work was to examine to what extent various
ture of utterances. They may fragment sentences or semantic units,
                                                                          kinds of lexical (word-based) information were useful for automatic
or group together spans of unrelated units. We examine several rea-
                                                                          linguistic segmentation. This precluded a study based on the out-
sons why such behavior is undesirable, and propose that linguistic
                                                                          put of existing speech recognition systems, which currently achieve
segmentations be used instead. This requires algorithms for auto-
                                                                          about 40-50% word error rate on the type of data used in our exper-
matically finding linguistic units. In this paper we report on first
                                                                          iments. At such high error rates, the analysis of any segmentation
results from our ongoing efforts toward such an automatic linguis-
                                                                          algorithm and the features it uses would likely be confounded by
tic segmentation. In all further discussion, unless otherwise noted,
                                                                          the unreliable nature of the input data. We therefore chose to elimi-
the terms ‘segment,’ ‘segmentation,’ etc. will refer to linguistic seg-
                                                                          nate the problem of inaccurate speech recognition and tested our al-
mentations.
                                                                          gorithms on hand-transcribed word-level transcripts of spontaneous
  2. THE IMPORTANCE OF LINGUISTIC                                         speech from the Switchboard corpus [4]. An additional benefit of
                                                                          this approach is that the models employed by the segmentation al-
            SEGMENTATION                                                  gorithms can also be directly used as language models for speech
Acoustic segmentations are inadequate in cases where the output           recognizers for the same type of data, an application we are pursu-
of a speech recognizer is to serve as input for further processing        ing as well.
based on syntactically or semantically coherent units. This includes
                                                                          The segmentation approaches we investigated all fell within the fol-
most natural language (NL) parsers or NL understanding or transla-
                                                                          lowing framework. We first trained a statistical language model
tion systems. For such systems, the fragmented recognition output
                                                                          of the N-gram variety to model the distribution of both words and
would have to be put back together and large spans of unrelated
                                                                          segment boundaries. (For this purpose, segment boundaries were
material would need to be resegmented into linguistic units.
                                                                          represented as special tokens <s> within the text.) The segmenta-
Automatic detection of linguistic segments could also improve the         tion information was removed from the test data, and the language
user interface of many speech systems. A spoken language system           model was used to hypothesize the most probable locations of seg-
ment boundaries. The resulting segmentations were then evaluated             computation yields the likelihoods of the states at each position k:
along a number of metrics.
As training data, we used 1.4 million words of Switchboard tran-                    PNO-S (w1 : : : wk )    =   PNO-S (w1 : : : wk 1 ) 
scripts annotated for linguistic segmentations by the UPenn Tree-                                                  p(wkjwk 2 wk 1 )
bank project [7], comprising a total of 193,000 segments. One half
of the standard Switchboard development test set, totaling 10,000
                                                                                                                +PS (w1 : : : wk 1 )   
words and 1,300 segments, was used for testing.                                                                    p(wkj<s>wk 1 )
The hand-annotated segments encompassed different kinds of lin-                         PS (w1 : : : wk )   =   PNO-S (w1 : : : wk 1 ) 
guistic structures, including                                                                                      p(<s>jwk 2 wk 1 ) p(wkj<s>)
                                                                                                                +PS (w1 : : : wk 1 )   
    Complete sentences                                                                                            p(<s>j<s>wk 1 ) p(wkj<s>)
    Stand-alone phrases
    Disfluent sentences aborted in mid-utterance1
    Interjections and back-channel responses                                A corresponding Viterbi algorithm is used to find the most likely
                                                                             sequence of S and NO-S (i.e., a segmentation) for a given word
The following excerpt illustrates the character of the data. Linguis-        string. This language model is a full implementation of the model
tic segment boundaries are marked <s>, whereas acoustic segmen-              approximated in [8]. The hidden disfluency model of [10] has a
tations are indicated by //.                                                 similar structure. As indicated in the formulae above, we currently
                                                                             use at most two words of history in the local conditional probabili-
      B.44: Worried that they're not going to                                ties p(j). Longer N-grams can be used if more state information is
      get enough attention? <s> //                                           kept.
      A.45: Yeah, <s> and, uh, you know, colds                               The local N-gram probabilities are estimated from the training data
      and things like that <laughter> get -- //                              by using Katz backoff with Good-Turing discounting [6].
      B.46:     Yeah.     <s> //
                                                                                                            5. RESULTS
      A.47: -- spread real easy and things,
      <s> but, // and they're expensive <s> and,                             5.1.      Baseline Segmentation Model
      // <lipsmack> // course, // there's a lot
      of different types of day care available,
                                                                             The first model we looked at models only plain words and segment
      too, // you know, where they teach them
                                                                             boundaries in the manner described. It was applied to the concate-
      academic things. <s> //
                                                                             nation of all turns of a conversation side, with no additional con-
                                                                             textual cues supplied. During testing, this model thus operates with
      B.48:     Yes.    <s> //
                                                                             very minimal information, i.e., with only the raw word sequence to
                                                                             be segmented. Table 1 shows results for bigram and trigram mod-
This short transcript shows some of the ubiquitous features of spon-
                                                                             els. The performance metrics used are defined as follows. Recall
taneous speech affecting segmentation, such as

    Mismatch between acoustic and linguistic segmentations
      (A.47)                                                                                 Table 1: Baseline model performance
                                                                                        Model     Recall Precision       FA     SER
    segments spanning several turns (A.45 and A.47)                                    Bigram    65.5%      56.9%      1.9% 58.9%
    backchannel responses (B.46)                                                       Trigram 70.2%        60.7%      2.0% 53.1%

                       4. THE MODEL
The language models used were of the N-gram type commonly used               is the percentage of actual segment boundaries hypothesized. Pre-
in speech recognition [5]. In N-gram models, a word wn from a                cision is the percentage of hypothesized boundaries that are actual.
n 1 word history w1 : : : wn 1. If the history contains a segment            False Alarms (FA) are the fraction of potential boundaries incor-
boundary <s>, it is truncated before that location. During testing,          rectly hypothesized as boundaries. Segment Error Rate (SER) is the
the model is run as a hidden segment model, hypothesizing segment            percentage of actual segments identified without intervening false
boundaries between any two words and implicitly computing the                alarms.
probabilities of all possible segmentations.                                 As can be seen, word context alone can identify a majority of seg-
Associated with each word position are two states, S and NO-S, cor-          ment boundaries at a modest false alarm rate of about 2%. The tri-
responding to a segment starting or not before that word. A forward          gram model does better than the bigram, but this is expected since it
                                                                             has access to a larger context around potential segment boundaries.
    1 Although complete and disfluent sentences were marked differently in   to use in its decision. Given these results, we only consider trigram
the corpus, we modeled these with a single type of boundary token.           models in all following experiments.
5.2.     Using Turn Information                                               boundaries provide some of the strongest cues for these boundaries.
                                                                              Apart from these strong lexical cues, it seems to be helpful to ab-
Next we examined a richer model that incorporated information
                                                                              stract from word identity and use POS information instead. In other
about the turn-taking between speakers.2 Note that turn boundaries
                                                                              words, the tag set could be optimized to provide the right level of
are already present in acoustic segmentations, but in this case we
                                                                              resolution for the segmentation task.
will only use them as a cue to the identification of linguistic seg-
ments. Turn information is easily incorporated into the segmenta-             It should be noted that the results for POS-based models are op-
tion model by placing special tags at turn boundaries (in both train-         timistic in the sense that for an actual application one would first
ing and testing). Model performance is summarized in Table 2.                 have to tag the input with POS labels, and then apply the segmenta-
                                                                              tion model. The actual performance would be degraded by tagging
                                                                              errors.
    Table 2: Segmentation performance using turn information
      Model          Recall Precision      FA       SER                       5.4.     Error Trade-offs
      Baseline       70.2%      60.7%     2.0% 53.1%                          As an aside to our search for useful features for the segmenta-
      Turn-tagged 76.9%         66.9%     1.8% 44.9%                          tion task, we observe that we can optimize any particular language
                                                                              model by trading off recall performance for false alarm rate, or vice
                                                                              versa. We did this by biasing the likelihoods of S states by some
As can be seen, adding turn information improves performance on               constant factor, causing the Viterbi algorithm to choose these states
all metrics. This improvement occurs even though turn boundaries              more often. Table 4 compares two bias values, and shows that the
are far from perfectly correlated with segment boundaries. As illus-          bias can be used to increase both recall and precision, while also
trated earlier, turns can contain multiple segments, or segments may          reducing the segment error rate.
span multiple turns.

5.3.     Using Part-of-Speech Information                                                         Table 4: Biasing segmentation
                                                                                       Model        Recall Precision      FA        SER
So far we have used only the identity of words. It is likely that                      Bias = 1     76.9%      66.9%     1.8%      44.9%
segmentation is closely related to syntactic (as opposed to lexical)                   Bias = 2     85.2%      69.2%     2.7%      37.4%
structure. Short of using a full-scale parser on the input we could
use the parts of speech (POS) of words as a more suitable represen-
tation from which to predict segment boundaries. Parts of speech
should also generalize much better to contexts containing N-grams                                   6. DISCUSSION
not observed in the training data (assuming the POS of the words
involved is known).                                                           6.1.     Error Analysis
We were able to test this hypothesis by using the POS-tagged ver-             To understand what type of errors the segmenter makes, we hand-
sion of the Switchboard corpus. We built two models based on POS              checked a set of 200 false alarms generated by the baseline trigram
from this data. Model I had all words replaced by their POS labels            model. The most frequent type (34%) of false alarm corresponded
during training and test, and also used turn boundary information.            to splitting of segments at sentence-internal clause boundaries, e.g.,
Model II also used POS labels, but retained the word identities of            false alarms triggered by a conjunction that would be likely to start
certain word classes that were deemed to be particularly relevant to          a segment. For example, the <s> in the segmentation
segmentation. These retained words include filled pauses, conjunc-
tions, and certain discourse markers such as “okay,” “so,” “well,”                   i'm not sure how many active volcanos
etc. Results are shown in Table 3.                                                   there are now <s> and and what the amount
                                                                                     of material that they do uh put into the
                                                                                     atmosphere
    Table 3: Segmentation performance using POS information
      Model           Recall Precision      FA     SER                        represents a false alarm, presumably triggered by the following co-
      Word-based      76.9%      66.9%     1.8% 44.9%                         ordinating conjunction “and.”
      POS-based I     68.9%      58.5%     2.0% 59.3%
                                                                              5% of the false alarms could be attributed to filled pauses at the
      POS-based II 79.6%         73.5%     0.9% 39.9%
                                                                              end of segments, which were often attached to the following seg-
                                                                              ment. This actually reflects a labeling ambiguity that should not be
                                                                              counted as an error. Another 7% of the false alarm we deemed to
We see that POS tags alone (Model I) do not result in better segmen-          be labeling errors. Thus, a total of 12% of false alarms could be
tations than words. The fact that Model II performs better than both          considered to be actually correct.
the all-word based model and the pure POS model indicates that
certain function words that tend to occur in the context of segment           6.2.     Other Segmentation Algorithms
    2 Speakers can talk over each other. We did not model this case sepa-     Our language-model-based segmentation algorithm is only one of
rately; instead, we adopted the serialization of turns implied by the tran-   many that could be used to perform the linguistic segmentation task,
scripts.                                                                      given a set of features. Conceptually, segmentation is just another
classification problem, in which each word transition must be la-                                8.    REFERENCES
beled as either a segment boundary or a within-segment transition.
                                                                               1. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A max-
Two natural choices for alternative approaches are decision trees
                                                                                  imum entropy approach to natural language processing. Com-
and a transformation-based, error-driven classifier of the type de-
                                                                                  putational Linguistics, 22(1):39–71, 1996.
veloped by Eric Brill for other tagging problems [2]. Both of these
methods would make it easier to combine diverse input features that            2. E. Brill. Some advances in transformation-based part of speech
are not readily integrated into a single probabilistic language model,            tagging. In Proceedings of the 12th National Conference on
e.g., if we wanted to use both POS and word identity for each word.3              Artificial Intelligence, Seattle, WA, 1994. AAAI Press.
Our approach, on the other hand, has the advantage of simplicity               3. K. W. Church. A stochastic parts program and noun phrase
and efficiency. Furthermore, the language model used for segmen-                  parser for unrestricted text. In Second Conference on Applied
tation can also be used for speech decoding or rescoring.                         Natural Language Processing, pages 136–143, Austin, Texas,
We already mentioned that if POS information is to be used for                    1988.
segmentation, an automatic tagging step is required. This presents             4. J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCH-
somewhat of a chicken-and-egg problem, in that taggers typically                  BOARD: Telephone speech corpus for research and develop-
rely on segmentations. An appealing solution to this problem in the               ment. In Proceedings IEEE Conference on Acoustics, Speech
statistical tagging framework [3] would be to model both segmen-                  and Signal Processing, volume I, pages 517–520, San Fran-
tation and tag assignment as a single hidden Markov process.                      cisco, March 1992.
6.3.     Other Features for Segmentation                                       5. F. Jelinek. Self-organized language modeling for speech recog-
                                                                                  nition. In A. Waibel and K.-F. Lee, editors, Readings in Speech
All of our experiments were based on lexical information only. To                 Recognition. Morgan Kaufmann, San Mateo, Ca., 1990.
further improve segmentation performance, and to make it less de-
pendent on accurate speech recognition, we plan to combine the LM              6. S. M. Katz. Estimation of probabilities from sparse data for
approach with a model for various acoustic and prosodic correlates                the language model component of a speech recognizer. IEEE
of segmentation. These include:                                                   Transactions on Acoustics, Speech, and Signal Processing,
                                                                                  35(3):400–401, March 1987.
    Unfilled pause durations                                                  7. M. Meteer et al. Dysfluency annotation stylebook for the
    Fundamental frequency patterns                                               Switchboard corpus. Distributed by LDC, February 1995. Re-
    Phone durations                                                              vised June 1995 by Ann Taylor.

    Glottalization                                                            8. M. Meteer and R. Iyer. Modeling conversational speech for
                                                                                  speech recognition. In Proceedings of the Conference on Em-
Our current segmentation model deals with each conversation side                  pirical Methods in Natural Language Processing, Philadelphia,
in isolation. An alternative approach is to model the two sides                   PA, May 1996.
jointly, thereby allowing us to capitalize on correlations between the         9. E. Shriberg and A. Stolcke. Word predictability after hesi-
segment structure of one speaker and what is said by the other. It is             tations: A corpus-based study. In Proceedings International
likely, for example, that backchannel responses would be modeled                  Conference on Spoken Language Processing, Philadelphia, PA,
better this way.                                                                  October 1996.

                    7. CONCLUSIONS                                            10. A. Stolcke and E. Shriberg. Statistical language modeling
                                                                                  for speech disfluencies. In Proceedings IEEE Conference on
We have argued for the need for automatic speech segmentation al-                 Acoustics, Speech and Signal Processing, volume I, pages 405–
gorithms that can identify linguistically motivated, sentence-level               408, Atlanta, GA, May 1996.
units of speech. We have shown that transcribed speech can be
segmented linguistically with good accuracy by using an N-gram
language model for the locations of the hidden segment boundaries.
We studied several word-level features for possible incorporation
in the model, and found that best performance so far was achieved
with a combination of function ‘cue’ words, POS labels, and turn
markers.

                      Acknowledgments
This research was supported by DARPA and NSF, under NSF Grant
IRI-9314967. The views herein are those of the authors and should
not be interpreted as representing the policies of DARPA or the
NSF.
   3 Such an integration can be achieved in a language model using the max-

imum entropy paradigm [1], but this would make the estimation process
considerably more expensive.
