le nom est : compression_phrases_Prog-Linear-jair.pdf

le titre est : Journal of Artificial Intelligence Research 31 (2008) 399-429  

les auteur sont : Edinburgh EH8 9LW, UK 

Abstract Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.  

l introduction est : 1. Introduction The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatical (Jing, 2000). A sentence compression mechanism would greatly benefit a wide range of applications. For example, in summarization, it could improve the conciseness of the generated summaries (Jing, 2000; Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). Other examples include compressing text to be displayed on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), subtitle generation from spoken transcripts (Vandeghinste & Pan, 2004), and producing audio scanning devices for the blind (Grefenstette, 1998). Sentence compression is commonly expressed as a word deletion problem: given an in- put source sentence of words x = x1, x2, . . . , xn, the aim is to produce a target compression by removing any subset of these words (Knight & Marcu, 2002). The compression prob- lem has been extensively studied across different modeling paradigms, both supervised and unsupervised. Supervised models are typically trained on a parallel corpus of source sen- tences and target compressions and come in many flavors. Generative models aim to model the probability of a target compression given the source sentence either directly (Galley & McKeown, 2007) or indirectly using the noisy-channel model (Knight & Marcu, 2002; Turner & Charniak, 2005), whereas discriminative formulations attempt to minimize error rate on a training set. These include decision-tree learning (Knight & Marcu, 2002), maxi- mum entropy (Riezler, King, Crouch, & Zaenen, 2003), support vector machines (Nguyen, Shimazu, Horiguchi, Ho, & Fukushi, 2004), and large-margin learning (McDonald, 2006). c 2008 AI Access Foundation. All rights reserved. Clarke & Lapata Unsupervised methods dispense with the parallel corpus and generate compressions either using rules (Turner & Charniak, 2005) or a language model (Hori & Furui, 2004). Despite differences in formulation, all these approaches model the compression process using local information. For instance, in order to decide which words to drop, they exploit information about adjacent words or constituents. Local models can do a good job at producing grammatical compressions, however they are somewhat limited in scope since they cannot incorporate global constraints on the compression output. Such constraints consider the sentence as a whole instead of isolated linguistic units (words or constituents). To give a concrete example we may want to ensure that each target compression has a verb, provided that the source had one in the first place. Or that verbal arguments are present in the compression. Or that pronouns are retained. Such constraints are fairly intuitive and can be used to instill not only linguistic but also task specific information into the model. For instance, an application which compresses text to be displayed on small screens would presumably have a higher compression rate than a system generating subtitles from spoken text. A global constraint could force the former system to generate compressions with a fixed rate or a fixed number of words. Existing approaches do not model global properties of the compression problem for a good reason. Finding the best compression for a source sentence given the space of all possible compressions1 (this search process is often referred to as decoding or inference) can become intractable for too many constraints and overly long sentences. Typically, the decoding problem is solved efficiently using dynamic programming often in conjunction with heuristics that reduce the search space (e.g., Turner & Charniak, 2005). Dynamic programming guarantees we will find the global optimum provided the principle of optimal- ity holds. This principle states that given the current state, the optimal decision for each of the remaining stages does not depend on previously reached stages or previously made decisions (Winston & Venkataramanan, 2003). However, we know this to be false in the case of sentence compression. For example, if we have included modifiers to the left of a noun in a compression then we should probably include the noun too or if we include a verb we should also include its arguments. With a dynamic programming approach we cannot easily guarantee such constraints hold. In this paper we propose a novel framework for sentence compression that incorporates constraints on the compression output and allows us to find an optimal solution. Our formulation uses integer linear programming (ILP), a general-purpose exact framework for NP-hard problems. Specifically, we show how previously proposed models can be recast as integer linear programs. We extend these models with constraints which we express as linear inequalities. Decoding in this framework amounts to finding the best solution given a linear (scoring) function and a set of linear constraints that can be either global or local. Although ILP has been previously used for sequence labeling tasks (Roth & Yih, 2004; Punyakanok, Roth, Yih, & Zimak, 2004), its application to natural language generation is less widespread. We present three compression models within the ILP framework, each representative of an unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui, 2004), and fully supervised modeling approach (McDonald, 2006). We propose a small number of constraints ensuring that the compressions are structurally and semantically 1. There are 2n possible compressions where n is the number of words in a sentence. 400 Global Inference for Sentence Compression valid and experimentally evaluate their impact on the compression task. In all cases, we show that the added constraints yield performance improvements. The remainder of this paper is organized as follows. Section 2 provides an overview of related work. In Section 3 we present the ILP framework and the compression models we employ in our experiments. Our constraints are introduced in Section 3.5. Section 4.3 discusses our experimental set-up and Section 5 presents our results. Discussion of future work concludes the paper. 

la conclusion est : (null)

ReferencesAho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3, 37–56.Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics for generation.In Proceedings of the first International Conference on Natural Language Generation,pp. 1–8, Mitzpe Ramon, Israel.Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for natural languagegeneration. In Proceedings of the Human Language Technology Conference of theNorth American Chapter of the Association for Computational Linguistics, pp. 359–366, New York, NY, USA.Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.In Proceedings of the 2006 Conference on Empirical Methods in Natural LanguageProcessing, pp. 189–198, Sydney, Australia.Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. InProceedings of the Third International Conference on Language Resources and Evaluation, pp. 1499–1504, Las Palmas, Gran Canaria.Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st NorthAmerican Annual Meeting of the Association for Computational Linguistics, pp. 132–139, Seattle, WA, USA.Clarke, J., & Lapata, M. (2006). Models for sentence compression: A comparison acrossdomains, training requirements and evaluation measures. In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pp. 377–384, Sydney, Australia.Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU–Cambridge toolkit. In Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece.426Global Inference for Sentence CompressionCormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. TheMIT Press.Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceedings of the Workshop on Automatic Summarization at the 2nd Meeting of the NorthAmerican Chapter of the Association for Computational Linguistics, pp. 89–98, Pittsburgh, PA, USA.Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3, 951–991.Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press,Princeton, NJ, USA.Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreferenceresolution using integer programming. In Human Language Technologies 2007: TheConference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pp. 236–243, Rochester, NY.Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D.thesis, Macquarie University.Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence compression.In In Proceedings of the North American Chapter of the Association for ComputationalLinguistics, pp. 180–187, Rochester, NY, USA.Gomory, R. E. (1960). Solving linear programming problems in integers. In Bellman,R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in AppliedMathematics, Vol. 10, Providence, RI, USA.Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction to Provide anAudio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.), Proceedingsof the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford,CA, USA.Hori, C., & Furui, S. (2004). Speech summarization: an approach through word extractionand a method for evaluation. IEICE Transactions on Information and Systems, E87D (1), 15–25.Jing, H. (2000). Sentence reduction for automatic text summarization. In Proceedings ofthe 6th Applied Natural Language Processing Conference, pp. 310–315, Seattle,WA,USA.Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: a probabilisticapproach to sentence compression. Artificial Intelligence, 139 (1), 91–107.Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programmingproblems. Econometrica, 28, 497–520.Lin, C.-Y. (2003). Improving summarization performance by sentence compression — a pilotstudy. In Proceedings of the 6th International Workshop on Information Retrieval withAsian Languages, pp. 1–8, Sapporo, Japan.Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings of the first HumanLanguage Technology Conference, pp. 222–227, San Francisco, CA, USA.427Clarke & LapataMarciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. InProceedings of the Ninth Conference on Computational Natural Language Learning,pp. 136–143, Ann Arbor, MI, USA.McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints.In Proceedings of the 11th Conference of the European Chapter of the Association forComputational Linguistics, Trento, Italy.McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with structured multilabel classification. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pp.987–994, Vancouver, BC, Canada.McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training of dependency parsers. In 43rd Annual Meeting of the Association for ComputationalLinguistics, pp. 91–98, Ann Arbor, MI, USA.Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and Combinatorial Optimization. WileyInterscience series in discrete mathematicals and opitmization. Wiley, New York, NY,USA.Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilisticsentence reduction using support vector machines. In Proceedings of the 20th international conference on Computational Linguistics, pp. 743–749, Geneva, Switzerland.Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). NumericalRecipes in C: The Art of Scientific Computing. Cambridge University Press, NewYork, NY, USA.Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. In Proceedings of the International Conference onComputational Linguistics, pp. 1346–1352, Geneva, Switzerland.Riedel, S., & Clarke, J. (2006). Incremental integer linear programming for non-projectivedependency parsing. In Proceedings of the 2006 Conference on Empirical Methods inNatural Language Processing, pp. 129–137, Sydney, Australia.Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensationusing ambiguity packing and stochastic disambiguation methods for lexical-functionalgrammar. In Human Language Technology Conference and the 3rd Meeting of theNorth American Chapter of the Association for Computational Linguistics, pp. 118–125, Edmonton, Canada.Roark, B. (2001). Probabilistic top-down parsing and language modeling. ComputationalLinguistics, 27 (2), 249–276.Roth, D. (1998). Learning to resolve natural language ambiguities: A unified approach. InIn Proceedings of the 15th of the American Association for Artificial Intelligence, pp.806–813, Madison, WI, USA.Roth, D., & Yih, W. (2004). A linear programming formulation for global inference innatural language tasks. In Proceedings of the Annual Conference on ComputationalNatural Language Learning, pp. 1–8, Boston, MA, USA.428Global Inference for Sentence CompressionRoth, D., & Yih, W. (2005). Integer linear programming inference for conditional randomfields. In Proceedings of the International Conference on Machine Learning, pp. 737–744, Bonn.Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields for information extraction. In Advances in Neural Information Processing Systems, Vancouver,BC, Canada.Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics, pp. 253–258,Helsinki, Finland.Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for sentencecompression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 290–297, Ann Arbor, MI, USA.Vandeghinste, V., & Pan, Y. (2004). Sentence compression for automated subtitling: Ahybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain.Williams, H. P. (1999). Model Building in Mathematical Programming (4th edition). Wiley.Winston, W. L., & Venkataramanan, M. (2003). Introduction to Mathematical Programming: Applications and Algorithms (4th edition). Duxbury.Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentencecompression as a tool for document summarization tasks. Information ProcessingManagement Special Issue on Summarization, 43 (6), 1549–1570.429

