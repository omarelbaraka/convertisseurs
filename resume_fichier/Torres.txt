le nom est : Torres.pdf

le titre est : Summary Evaluation with and without References 

les auteur sont :                                 with and without References 

Abstract—We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called F RESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as C OVERAGE, R ESPONSIVENESS, P YRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.  

l introduction est : I. INTRODUCTION TEXT summarization evaluation has always been a complex and controversial issue in computational linguistics. In the last decade, significant advances have been made in this field as well as various evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The first one, SUMMAC, ran from 1996 to 1998 under the auspices of the Tipster program [1], and the second one, entitled DUC (Document Understanding Conference) [2], was the main evaluation forum from 2000 until 2007. Nowadays, the Text Analysis Conference (TAC) [3] provides a forum for assessment of different information access technologies including text summarization. Evaluation in text summarization can be extrinsic or intrinsic [4]. In an extrinsic evaluation, the summaries are assessed in the context of an specific task carried out by a human or a machine. In an intrinsic evaluation, the summaries are evaluated in reference to some ideal model. SUMMAC was mainly extrinsic while DUC and TAC followed an intrinsic evaluation paradigm. In an intrinsic evaluation, an Manuscript received June 8, 2010. Manuscript accepted for publication July 25, 2010. Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon, France and École Polytechnique de Montréal, Canada (juan-manuel.torres@univ-avignon.fr). Eric SanJuan is with LIA/Université d’Avignon, France (eric.sanjuan@univ-avignon.fr). Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain (horacio.saggion@upf.edu). Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain; LIA/Université d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico (iria.dacunha@upf.edu). Patricia Velázquez-Morales is with VM Labs, France (patricia velazquez@yahoo.com). automatically generated summary (peer) has to be compared with one or more reference summaries (models). DUC used an interface called SEE to allow human judges to compare a peer with a model. Thus, judges give a COVERAGE score to each peer produced by a system and the final system COVERAGE score is the average of the COVERAGE’s scores asigned. These system’s COVERAGE scores can then be used to rank summarization systems. In the case of query-focused summarization (e.g. when the summary should answer a question or series of questions) a RESPONSIVENESS score is also assigned to each summary, which indicates how responsive the summary is to the question(s). Because manual comparison of peer summaries with model summaries is an arduous and costly process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries [5]. Various vocabulary overlap measures such as n-grams overlap or longest common subsequence between peer and model have also been proposed [6], [7]. The BLEU machine translation evaluation measure [8] has also been tested in summarization [9]. The DUC conferences adopted the ROUGE package for content-based evaluation [10]. ROUGE implements a series of recall measures based on n-gram co-occurrence between a peer summary and a set of model summaries. These measures are used to produce systems’ rank. It has been shown that system rankings, produced by some ROUGE measures (e.g., ROUGE-2, which uses 2-grams), have a correlation with rankings produced using COVERAGE. In recent years the PYRAMIDS evaluation method [11] has been introduced. It is based on the distribution of “content” of a set of model summaries. Summary Content Units (SCUs) are first identified in the model summaries, then each SCU receives a weight which is the number of models containing or expressing the same unit. Peer SCUs are identified in the peer, matched against model SCUs, and weighted accordingly. The PYRAMIDS score given to a peer is the ratio of the sum of the weights of its units and the sum of the weights of the best possible ideal summary with the same number of SCUs as the peer. The PYRAMIDS scores can be also used for ranking summarization systems. [11] showed that PYRAMIDS scores produced reliable system rankings when multiple (4 or more) models were used and that PYRAMIDS rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE with skip 2-grams). However, this method requires the creation 13 Polibits (42) 2010 of models and the identification, matching, and weighting of SCUs in both: models and peers. [12] evaluated the effectiveness of the Jensen-Shannon (J S) [13] theoretic measure in predicting systems ranks in two summarization tasks: query-focused and update summarization. They have shown that ranks produced by PYRAMIDS and those produced by J S measure correlate. However, they did not investigate the effect of the measure in summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other than English. In this paper we present a series of experiments aimed at a better understanding of the value of the J S divergence for ranking summarization systems. We have carried out experimentation with the proposed measure and we have verified that in certain tasks (such as those studied by [12]) there is a strong correlation among PYRAMIDS, RESPONSIVENESS and the J S divergence, but as we will show in this paper, there are datasets in which the correlation is not so strong. We also present experiments in Spanish and French showing positive correlation between the J S and ROUGE which is the de facto evaluation measure used in evaluation of non-English summarization. To the best of our knowledge this is the more extensive set of experiments interpreting the value of evaluation without human models. The rest of the paper is organized in the following way: First in Section II we introduce related work in the area of content-based evaluation identifying the departing point for our inquiry; then in Section III we explain the methodology adopted in our work and the tools and resources used for experimentation. In Section IV we present the experiments carried out together with the results. Section V discusses the results and Section VI concludes the paper and identifies future work. 

la conclusion est : 

R EFERENCESp-valuep < 0.05p < 0.05p < 0.10p < 0.05ROUGE -SU40.7410.6800.6200.740p-valuep < 0.01p < 0.02p < 0.05p < 0.01[18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,“A French Human Reference Corpus for multi-documentssummarization and sentence compression,” in LREC’10, vol. 2,Malta, 2010, p. In press.[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energyof Associative Memories: performants applications of Enertex algorithmin text summarization and topic segmentation,” in MICAI’07, 2007, pp.861–871.[20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,St Malo, France, 2002, pp. 723–734.[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,“Automatic summarization using terminological and semanticresources,” in LREC’10, vol. 2, Malta, 2010, p. In press.[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme gloutonappliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,p. In press.[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modernsystems of automatic text summarization,” Automatic Documentationand Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.[24] C. D. Manning and H. Schütze, Foundations of Statistical NaturalLanguage Processing.Cambridge, Massachusetts: The MIT Press,1999.[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,vol. 43, no. 6, pp. 1449–1481, 2007.[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specializeddiscourse: The case of medical articles in spanish,” Terminology, vol. 13,no. 2, pp. 249–286, 2007.[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACLStudent Research Workshop.Toulouse, France: Association forComputational Linguistics, 9-11 July 2001 2001, pp. 49–54.[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,Singapore, August 2009, pp. 23–30.[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:Sentence compression,” in Proceedings of the National Conference onArtificial Intelligence. Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999, 2000, pp. 703–710.[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB. Sundheim, “Summac: a text summarization evaluation,” NaturalLanguage Engineering, vol. 8, no. 1, pp. 43–68, 2002.[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,no. 6, pp. 1506–1520, 2007.[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,USA: NIST, November 17-19 2008.[4] K. Spärck Jones and J. Galliers, Evaluating Natural LanguageProcessing Systems, An Analysis and Review, ser. Lecture Notes inComputer Science. Springer, 1996, vol. 1083.[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison ofrankings produced by summarization evaluation measures,” in NAACLWorkshop on Automatic Summarization, 2000, pp. 69–78.[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluationof Summaries in a Cross-lingual Environment using Content-basedMetrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,D. Liu, and E. Drábek, “Evaluation challenges in large-scale documentsummarization,” in ACL’03, 2003, pp. 375–382.[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a methodfor automatic evaluation of machine translation,” in ACL’02, 2002, pp.311–318.[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in EvaluationInitiatives in Natural Language Processing. Budapest, Hungary: EACL,14 April 2003.[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation ofSummaries,” in Text Summarization Branches Out: ACL-04 Workshop,M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection inSummarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.145–152.[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selectionin Summarization without Human Models,” in Empirical Methods inNatural Language Processing, Singapore, August 2009, pp. 306–314.[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEETransactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries UsingN-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,USA: Association for Computational Linguistics, 2003, pp. 71–78.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoreticapproach to automatic evaluation of summaries,” in HLT-NAACL,Morristown, USA, 2006, pp. 463–470.[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. ofMath. Stat., vol. 22, no. 1, pp. 79–86, 1951.[17] S. Siegel and N. Castellan, Nonparametric Statistics for the BehavioralSciences. McGraw-Hill, 1998.19Polibits (42) 2010

