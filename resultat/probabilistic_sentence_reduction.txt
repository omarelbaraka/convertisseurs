Probabilistic Sentence Reduction Using Support Vector Machines
Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi
Bao Tu Ho and Masaru Fukushi
Japan Advanced Institute of Science and Technology
1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN
{nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp

Abstract
This paper investigates a novel application of support vector machines (SVMs) for sentence reduction.
We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed
methods outperform earlier methods in term of sentence reduction performance.

1

Introduction

The most popular methods of sentence reduction for text summarization are corpus based
methods. Jing (Jing 00) developed a method
to remove extraneous phrases from sentences
by using multiple sources of knowledge to decide which phrases could be removed. However,
while this method exploits a simple model for
sentence reduction by using statistics computed
from a corpus, a better model can be obtained
by using a learning approach.
Knight and Marcu (Knight and Marcu 02)
proposed a corpus based sentence reduction
method using machine learning techniques.
They discussed a noisy-channel based approach
and a decision tree based approach to sentence
reduction. Their algorithms provide the best
way to scale up the full problem of sentence reduction using available data. However, these algorithms require that the word order of a given
sentence and its reduced sentence are the same.
Nguyen and Horiguchi (Nguyen and Horiguchi
03) presented a new sentence reduction technique based on a decision tree model without
that constraint. They also indicated that semantic information is useful for sentence reduction tasks.
The major drawback of previous works on
sentence reduction is that those methods are
likely to output local optimal results, which may
have lower accuracy. This problem is caused by
the inherent sentence reduction model; that is,
only a single reduced sentence can be obtained.

As pointed out by Lin (Lin 03), the best sentence reduction output for a single sentence is
not approximately best for text summarization.
This means that “local optimal” refers to the
best reduced output for a single sentence, while
the best reduced output for the whole text is
“global optimal”. Thus, it would be very valuable if the sentence reduction task could generate multiple reduced outputs and select the best
one using the whole text document. However,
such a sentence reduction method has not yet
been proposed.
Support Vector Machines (Vapnik 95), on the
other hand, are strong learning methods in comparison with decision tree learning and other
learning methods (Sholkopf 97). The goal of
this paper is to illustrate the potential of SVMs
for enhancing the accuracy of sentence reduction in comparison with previous work. Accordingly, we describe a novel deterministic method
for sentence reduction using SVMs and a twostage method using pairwise coupling (Hastie
98). To solve the problem of generating multiple best outputs, we propose a probabilistic
sentence reduction model, in which a variant of
probabilistic SVMs using a two-stage method
with pairwise coupling is discussed.
The rest of this paper will be organized as
follows: Section 2 introduces the Support Vector Machines learning. Section 3 describes the
previous work on sentence reduction and our
deterministic sentence reduction using SVMs.
We also discuss remaining problems of deterministic sentence reduction. Section 4 presents
a probabilistic sentence reduction method using
support vector machines to solve this problem.
Section 5 discusses implementation and our experimental results; Section 6 gives our conclusions and describes some problems that remain
to be solved in the future.

2

Support Vector Machine

Support vector machine (SVM)(Vapnik 95) is a
technique of machine learning based on statistical learning theory. Suppose that we are given
l training examples (xi , yi ), (1 ≤ i ≤ l), where
xi is a feature vector in n dimensional feature
space, yi is the class label {-1, +1 } of xi . SVM
finds a hyperplane w.x + b = 0 which correctly
separates the training examples and has a maximum margin which is the distance between two
hyperplanes w.x + b ≥ 1 and w.x + b ≤ −1. The
optimal hyperplane with maximum margin can
be obtained by solving the following quadratic
programming.
1
2 kwk + C0

min

l
P
i

ξi

(1)

s.t.
yi (w.xi + b) ≥ 1 − ξi
ξi ≥ 0

where C0 is the constant and ξi is a slack variable for the non-separable case. In SVM, the
optimal hyperplane is formulated as follows:
f (x) = sign

Ã l
X

!
αi yi K(xi , x) + b

(2)

1

where αi is the Lagrange multiple, and
K(x0 , x00 ) is a kernel function, the SVM calculates similarity between two arguments x0 and
x00 . For instance, the Polynomial kernel function is formulated as follow:
K(x0 , x00 ) = (x0 .x00 )p

(3)

SVMs estimate the label of an unknown example x whether the sign of f (x) is positive or
not.

3

Deterministic Sentence Reduction
Using SVMs

3.1 Problem Description
In the corpus-based decision tree approach, a
given input sentence is parsed into a syntax tree
and the syntax tree is then transformed into a
small tree to obtain a reduced sentence.
Let t and s be syntax trees of the original sentence and a reduced sentence, respectively. The
process of transforming syntax tree t to small
tree s is called “rewriting process” (Knight and
Marcu 02), (Nguyen and Horiguchi 03). To
transform the syntax tree t to the syntax tree
s, some terms and five rewriting actions are defined.
An Input list consists of a sequence of words
subsumed by the tree t where each word in the
Input list is labelled with the name of all syntactic constituents in t. Let CSTACK be a stack

that consists of sub trees in order to rewrite a
small tree. Let RSTACK be a stack that consists of sub trees which are removed from the
Input list in the rewriting process.
• SHIFT action transfers the first word from the
Input list into CSTACK. It is written mathematically and given the label SHIFT.
• REDUCE(lk, X) action pops the lk syntactic
trees located at the top of CSTACK and combines them in a new tree, where lk is an integer
and X is a grammar symbol.
• DROP X action moves subsequences of words
that correspond to syntactic constituents from
the Input list to RSTACK.
• ASSIGN TYPE X action changes the label of
trees at the top of the CSTACK. These POS
tags might be different from the POS tags in
the original sentence.
• RESTORE X action takes the X element in
RSTACK and moves it into the Input list,
where X is a subtree.

For convenience, let configuration be a status
of Input list, CSTACK and RSTACK. Let current context be the important information in a
configuration. The important information are
defined as a vector of features using heuristic
methods as in (Knight and Marcu 02), (Nguyen
and Horiguchi 03).
The main idea behind deterministic sentence
reduction is that it uses a rule in the current
context of the initial configuration to select a
distinct action in order to rewrite an input sentence into a reduced sentence. After that, the
current context is changed to a new context and
the rewriting process is repeated for selecting
an action that corresponds to the new context.
The rewriting process is finished when it meets
a termination condition. Here, one rule corresponds to the function that maps the current
context to a rewriting action. These rules are
learned automatically from the corpus of long
sentences and their reduced sentences (Knight
and Marcu 02), (Nguyen and Horiguchi 03).
3.2 Example
Figure 1 shows an example of applying a sequence of actions to rewrite the input sentence
(a, b, c, d, e), when each character is a word. It
illustrates the structure of the Input list, two
stacks, and the term of a rewriting process based
on the actions mentioned above. For example,
in the first row, DROP H deletes the sub-tree
with its root node H in the Input list and stores

it in the RSTACK. The reduced tree s can be
obtained after applying a sequence of actions
as follows: DROP H; SHIFT; ASSIGN TYPE K;
DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
F; RESTORE H; SHIFT; ASSIGN TYPE D; REDUCE 2G. In this example, the reduced sentence

is (b, e, a).

Figure 2: Example of Configuration
that start with the first unit in the Input list.

Figure 1: An Example of the Rewriting Process
3.3

Learning Reduction Rules Using
SVMs
As mentioned above, the action for each configuration can be decided by using a learning rule,
which maps a context to an action. To obtain
such rules, the configuration is represented by
a vector of features with a high dimension. After that, we estimate the training examples by
using several support vector machines to deal
with the multiple classification problem in sentence reduction.
3.3.1 Features
One important task in applying SVMs to text
summarization is to define features. Here, we
describe features used in our sentence reduction
models.
The features are extracted based on the current context. As it can be seen in Figure 2, a
context includes the status of the Input list and
the status of CSTACK and RSTACK. We define a set of features for a current context as
described bellow.
Operation feature
The set of features as described in (Nguyen and
Horiguchi 03) are used in our sentence reduction
models.
Original tree features
These features denote the syntactic constituents

For example, in Figure 2 the syntactic constituents are labels of the current element in the
Input list from “VP” to the verb “convince”.
Semantic features
The following features are used in our model as
semantic information.
• Semantic information about current words
within the Input list; these semantic types
are obtained by using the named entities such
as Location, Person, Organization and Time
within the input sentence. To define these
name entities, we use the method described in
(Borthwick 99).
• Semantic information about whether or not the
word in the Input list is a head word.
• Word relations, such as whether or not a word
has a relationship with other words in the subcategorization table. These relations and the
sub-categorization table are obtained using the
Commlex database (Macleod 95).

Using the semantic information, we are able to
avoid deleting important segments within the
given input sentence. For instance, the main
verb, the subject and the object are essential
and for the noun phrase, the head noun is essential, but an adjective modifier of the head noun
is not. For example, let us consider that the
verb “convince” was extracted from the Comlex database as follows.
convince
NP-PP: PVAL (“of”)
NP-TO-INF-OC

This entry indicates that the verb “convince”

can be followed by a noun phrase and a prepositional phrase starting with the preposition “of”.
It can be also followed by a noun phrase and a
to-infinite phrase. This information shows that
we cannot delete an “of” prepositional phrase
or a to-infinitive that is the part of the verb
phrase.
Two-stage SVM Learning using
Pairwise Coupling
Using these features we can extract training
data for SVMs. Here, a sample in our training
data consists of pairs of a feature vector and
an action. The algorithm to extract training
data from the training corpus is modified using
the algorithm described in our pervious work
(Nguyen and Horiguchi 03).
Since the original support vector machine
(SVM) is a binary classification method, while
the sentence reduction problem is formulated as
multiple classification, we have to find a method
to adapt support vector machines to this problem. For multi-class SVMs, one can use strategies such as one-vs all, pairwise comparison or
DAG graph (Hsu 02). In this paper, we use the
pairwise strategy, which constructs a rule for
discriminating pairs of classes and then selects
the class with the most winning among two class
decisions.
To boost the training time and the sentence
reduction performance, we propose a two-stage
SVM described below.
Suppose that the examples in training data
are divided into five groups m1 , m2 , ..., m5 according to their actions. Let Svmc be multiclass SVMs and let Svmc-i be multi-class SVMs
for a group mi . We use one Svmc classifier to
identify the group to which a given context e
should be belong. Assume that e belongs to
the group mi . The classifier Svmc-i is then used
to recognize a specific action for the context e.
The five classifiers Svmc-1, Svmc-2,..., Svmc-5
are trained by using those examples which have
actions belonging to SHIFT, REDUCE, DROP,
ASSIGN TYPE and RESTORE.
Table 1 shows the distribution of examples in
five data groups.

Table 1: Distribution of example data on five
data groups
Name
SHIFT-GROUP
REDUCE-GROUP
DROP-GROUP
ASSIGN-GROUP
RESTORE-GROUP
TOTAL

3.3.2

3.4

Disadvantage of Deterministic
Sentence Reductions
The idea of the deterministic algorithm is to
use the rule for each current context to select
the next action, and so on. The process terminates when a stop condition is met. If the early
steps of this algorithm fail to select the best ac-

Number of examples
13,363
11,406
4,216
13,363
2,004
44,352

tions, then the possibility of obtaining a wrong
reduced output becomes high.
One way to solve this problem is to select multiple actions that correspond to the context at
each step in the rewriting process. However,
the question that emerges here is how to determine which criteria to use in selecting multiple
actions for a context. If this problem can be
solved, then multiple best reduced outputs can
be obtained for each input sentence and the best
one will be selected by using the whole text document.
In the next section propose a model for selecting multiple actions for a context in sentence
reduction as a probabilistic sentence reduction
and present a variant of probabilistic sentence
reduction.

4

Probabilistic Sentence Reduction
Using SVM

4.1
The Probabilistic SVM Models
Let A be a set of k actions A =
{a1 , a2 ...ai , ..., ak } and C be a set of n contexts C = {c1 , c2 ...ci , ..., cn } . A probabilistic
model α for sentence reduction will select an
action a ∈ A for the context c with probability
pα (a|c). The pα (a|c) can be used to score action a among possible actions A depending the
context c that is available at the time of decision. There are several methods for estimating
such scores; we have called these “probabilistic
sentence reduction methods”. The conditional
probability pα (a|c) is estimated using a variant
of probabilistic support vector machine, which
is described in the following sections.
4.1.1 Probabilistic SVMs using
Pairwise Coupling
For convenience, we denote uij = p(a = ai |a =
ai ∨aj , c). Given a context c and an action a, we
assume that the estimated pairwise class probabilities rij of uij are available. Here rij can
be estimated by some binary classifiers. For
instance, we could estimate rij by using the

SVM binary posterior probabilities as described
in (Plat 2000). Then, the goal is to estimate
{pi }ki=1 , where pi = p(a = ai |c), i = 1, 2, ..., k.
For this propose, a simple estimate of these
probabilities can be derived using the following
voting method:
X

pi = 2

the search procedure ranks the derivation of incomplete and complete reduced sentences. Let
d(s) = {a1 , a2 , ...ad } be the derivation of a small
tree s, where each action ai belongs to a set of
possible actions. The score of s is the product
of the conditional probabilities of the individual
actions in its derivation.

I{rij >rji } /k(k − 1)

X

nij rij log

i6=j

rij
uij

(4)

where rij and uij are the probabilities of a pairwise ai and aj in the estimated model and in
our model, respectively, and nij is the number
of training data in which their classes are ai or
aj . To find the minimizer of equation (6), they
first calculate
∂l(p) X
rij
1
=
nij (−
+
).
∂pi
pi
pi + pj
i6=j

Thus, letting ∆l(p) = 0, they proposed to find
a point satisfying
X
j:j6=i

nij uij =

X
j:j6=i

nij rij ,

k
P
i=1

pi = 1,

where i = 1, 2, ...k and pi > 0.
Such a point can be obtained by using an algorithm described elsewhere in (Hastie 98). We
applied it to obtain a probabilistic SVM model
for sentence reduction using a simple method as
follows. Assume that our class labels belong to
l groups: M = {m1 , m2 ...mi , ..., ml } , where l
is a number of groups and mi is a group e.g.,
SHIFT, REDUCE ,..., ASSIGN TYPE. Then
the probability p(a|c) of an action a for a given
context c can be estimated as follows.
p(a|c) = p(mi |c) × p(a|c, mi )

(5)

where mi is a group and a ∈ mi . Here, p(mi |c)
and p(a|c, mi ) are estimated by the method in
(Hastie 98).
4.2

p(ai |ci )

(6)

ai ∈d(s)

where I is an indicator function and k(k − 1) is
the number of pairwise classes. However, this
model is too simple; we can obtain a better one
with the following method.
Assume that uij are pairwise probabilities of
the model subject to the condition that uij =
pi /(pi +pj ). In (Hastie 98), the authors proposed
to minimize the Kullback-Leibler (KL) distance
between the rij and uij
l(p) =

Y

Score(s) =

j:j6=i

Probabilistic sentence reduction
algorithm
After obtaining a probabilistic model p, we then
use this model to define function score, by which

where ci is the context in which ai was decided.
The search heuristic tries to find the best reduced tree s∗ as follows:
s∗ = argmax
| {z }

Score(s)

(7)

s∈tree(t)

where tree(t) are all the complete reduced trees
from the tree t of the given long sentence. Assume that for each configuration the actions
{a1 , a2 , ...an } are sorted in decreasing order according to p(ai |ci ), in which ci is the context
of that configuration. Algorithm 1 shows a
probabilistic sentence reduction using the top
K-BFS search algorithm. This algorithm uses
a breadth-first search which does not expand
the entire frontier, but instead expands at most
the top K scoring incomplete configurations in
the frontier; it is terminated when it finds M
completed reduced sentences (CL is a list of reduced trees), or when all hypotheses have been
exhausted. A configuration is completed if and
only if the Input list is empty and there is one
tree in the CSTACK. Note that the function
get-context(hi , j) obtains the current context of
the j th configuration in hi , where hi is a heap at
step i. The function Insert(s,h) ensures that the
heap h is sorted according to the score of each
element in h. Essentially, in implementation we
can use a dictionary of contexts and actions observed from the training data in order to reduce
the number of actions to explore for a current
context.

5

Experiments and Discussion

We used the same corpus as described in
(Knight and Marcu 02), which includes 1,067
pairs of sentences and their reductions. To
evaluate sentence reduction algorithms, we randomly selected 32 pairs of sentences from our
parallel corpus, which is refered to as the test
corpus. The training corpus of 1,035 sentences
extracted 44,352 examples, in which each training example corresponds to an action. The
SVM tool, LibSVM (Chang 01) is applied to
train our model. The training examples were

Algorithm 1 A probabilistic sentence reduction
algorithm
1: CL={Empty};
i = 0; h0 ={ Initial configuration}

2: while |CL| < M do
3:
if hi is empty then
4:
break;
5:
end if
6:
u =min(|hi |, K)
7:
for j = 1 to u do
8:
c=get-context(hi , j)
9:
10:
11:
12:

Select m so that

m
P

p(ai |c) < Q is maximal

i=1

for l=1 to m do
parameter=get-parameter(al );
Obtain a new configuration s by performing action al
with parameter

if Complete(s) then
13:
14:
Insert(s, CL)
15:
else
16:
Insert(s, hi+1 )
17:
end if
18:
end for
19:
end for
20:
i=i+1
21: end while

divided into SHIFT, REDUCE, DROP, RESTORE, and ASSIGN groups. To train our
support vector model in each group, we used
the pairwise method with the polynomial kernel function, in which the parameter p in (3)
and the constant C0 in equation (1) are 2 and
0.0001, respectively.
The algorithms (Knight and Marcu 02) and
(Nguyen and Horiguchi 03) served as the baseline1 and the baseline2 for comparison with the
proposed algorithms. Deterministic sentence reduction using SVM and probabilistic sentence
reduction were named as SVM-D and SVMP, respectively. For convenience, the ten top reduced
outputs using SVMP were called SVMP-10. We
used the same evaluation method as described
in (Knight and Marcu 02) to compare the proposed methods with previous methods. For this
experiment, we presented each original sentence
in the test corpus to three judges who are specialists in English, together with three sentence
reductions: the human generated reduction sentence, the outputs of the proposed algorithms,
and the output of the baseline algorithms.
The judges were told that all outputs were
generated automatically. The order of the outputs was scrambled randomly across test cases.
The judges participated in two experiments. In
the first, they were asked to determine on a scale
from 1 to 10 how well the systems did with respect to selecting the most important words in
the original sentence. In the second, they were
asked to determine the grammatical criteria of

reduced sentences.
Table 2 shows the results of English language
sentence reduction using a support vector machine compared with the baseline methods and
with human reduction. Table 2 shows compression rates, and mean and standard deviation results across all judges, for each algorithm. The
results show that the length of the reduced sentences using decision trees is shorter than using
SVMs, and indicate that our new methods outperform the baseline algorithms in grammatical
and importance criteria. Table 2 shows that the
Table 2: Experiment results with Test Corpus
Method
Baseline1
Baseline2
SVM-D
SVMP-10
Human

Comp
57.19%
57.15%
57.65%
57.51%
64.00%

Gramma
8.60 ± 2.8
8.60 ± 2.1
8.76 ± 1.2
8.80 ± 1.3
9.05 ± 0.3

Impo
7.18 ± 1.92
7.42 ± 1.90
7.53 ± 1.53
7.74 ± 1.39
8.50 ± 0.80

first 10 reduced sentences produced by SVMP10 (the SVM probabilistic model) obtained the
highest performances. We also compared the
computation time of sentence reduction using
support vector machine with that in previous
works. Table 3 shows that the computational
times for SVM-D and SVMP-10 are slower than
baseline, but it is acceptable for SVM-D.
Table 3: Computational times of performing reductions on test-set. Average sentence length
was 21 words.
Method
Baseline1
SVM-D
SVMP-10

Computational times (sec)
138.25
212.46
1030.25

We also investigated how sensitive the proposed algorithms are with respect to the training data by carrying out the same experiment on sentences of different genres. We
created the test corpus by selecting sentences
from the web-site of the Benton Foundation
(http://www.benton.org). The leading sentences in each news article were selected as the
most relevant sentences to the summary of the
news. We obtained 32 leading long sentences
and 32 headlines for each item. The 32 sentences are used as a second test for our methods.
We use a simple ranking criterion: the more the
words in the reduced sentence overlap with the
words in the headline, the more important the

sentence is. A sentence satisfying this criterion
is called a relevant candidate.
For a given sentence, we used a simple
method, namely SVMP-R to obtain a reduced sentence by selecting a relevant candidate among the ten top reduced outputs using
SVMP-10.
Table 4 depicts the experiment results for
the baseline methods, SVM-D, SVMP-R, and
SVMP-10. The results shows that, when applied to sentence of a different genre, the performance of SVMP-10 degrades smoothly, while
the performance of the deterministic sentence
reductions (the baselines and SVM deterministic) drops sharply. This indicates that the probabilistic sentence reduction using support vector
machine is more stable.
Table 4 shows that the performance of
SVMP-10 is also close to the human reduction
outputs and is better than previous works. In
addition, SVMP-R outperforms the deterministic sentence reduction algorithms and the differences between SVMP-R’s results and SVMP10 are small. This indicates that we can obtain reduced sentences which are relevant to the
headline, while ensuring the grammatical and
the importance criteria compared to the original sentences.
Table 4: Experiment results with Benton Corpus
Method
Baseline1
Baseline2
SVM-D
SVMP-R
SVMP-10
Human

6

Comp
54.14%
53.13%
56.64%
58.31%
57.62%
64.00%

Gramma
7.61 ± 2.10
7.72 ± 1.60
7.86 ± 1.20
8.25 ± 1.30
8.60 ± 1.32
9.01 ± 0.25

Impo
6.74 ± 1.92
7.02 ± 1.90
7.23 ± 1.53
7.54 ± 1.39
7.71 ± 1.41
8.40 ± 0.60

Conclusions

We have presented a new probabilistic sentence
reduction approach that enables a long sentence
to be rewritten into reduced sentences based on
support vector models. Our methods achieves
better performance when compared with earlier
methods. The proposed reduction approach can
generate multiple best outputs. Experimental
results showed that the top 10 reduced sentences
returned by the reduction process might yield
accuracies higher than previous work. We believe that a good ranking method might improve
the sentence reduction performance further in a
text.

References
A. Borthwick, “A Maximum Entropy Approach
to Named Entity Recognition”, Ph.D thesis, Computer Science Department, New York
University (1999).
C.-C. Chang and C.-J. Lin,
“LIBSVM: a library for support vector machines”, Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
H. Jing, “Sentence reduction for automatic
text summarization”, In Proceedings of the
First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL-2000.
T.T. Hastie and R. Tibshirani, “Classification
by pairwise coupling”, The Annals of Statistics, 26(1): pp. 451-471, 1998.
C.-W. Hsu and C.-J. Lin, “A comparison of
methods for multi-class support vector machines”, IEEE Transactions on Neural Networks, 13, pp. 415-425, 2002.
K. Knight and D. Marcu, “Summarization beyond sentence extraction: A Probabilistic approach to sentence compression”, Artificial
Intelligence 139: pp. 91-107, 2002.
C.Y. Lin, “Improving Summarization Performance by Sentence Compression — A Pilot Study”, Proceedings of the Sixth International Workshop on Information Retrieval
with Asian Languages, pp.1-8, 2003.
C. Macleod and R. Grishman, “COMMLEX
syntax Reference Manual”; Proteus Project,
New York University (1995).
M.L. Nguyen and S. Horiguchi, “A new sentence
reduction based on Decision tree model”,
Proceedings of 17th Pacific Asia Conference
on Language, Information and Computation,
pp. 290-297, 2003
V. Vapnik, “The Natural of Statistical Learning
Theory”, New York: Springer-Verlag, 1995.
J. Platt,“ Probabilistic outputs for support vector machines and comparison to regularized
likelihood methods,” in Advances in Large
Margin Classifiers, Cambridege, MA: MIT
Press, 2000.
B. Scholkopf et al, “Comparing Support Vector Machines with Gausian Kernels to Radius
Basis Function Classifers”, IEEE Trans. Signal Procesing, 45, pp. 2758-2765, 1997.

